Title,Date,Url,Author
Knowledge Distillation vs. Pretraining from Scratch under a Fixed (Computation) Budget,2024-04-30,http://arxiv.org/abs/2404.19319,"Bui, Minh Duc; Schmidt, Fabian David; Glava≈°, Goran; von der Wense, Katharina"
Why are Adaptive Methods Good for Attention Models?,2020-10-23,http://arxiv.org/abs/1912.03194,"Zhang, Jingzhao; Karimireddy, Sai Praneeth; Veit, Andreas; Kim, Seungyeon; Reddi, Sashank J.; Kumar, Sanjiv; Sra, Suvrit"
