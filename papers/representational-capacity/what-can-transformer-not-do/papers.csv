Title,Date,Url,Author
Collapse of Self-trained Language Models,2024-04-02,http://arxiv.org/abs/2404.02305,David Herel; Tomas Mikolov
The pitfalls of next-token prediction,2024-03-11,http://arxiv.org/abs/2403.06963,Gregor Bachmann; Vaishnavh Nagarajan
Why are Sensitive Functions Hard for Transformers?,2024-03-03,http://arxiv.org/abs/2402.09963,Michael Hahn; Mark Rofin
"Transformers are Expressive, But Are They Expressive Enough for Regression?",2024-02-23,http://arxiv.org/abs/2402.15478,Swaroop Nath; Harshad Khadilkar; Pushpak Bhattacharyya
Limits of Transformer Language Models on Learning Algorithmic Compositions,2024-02-13,http://arxiv.org/abs/2402.05785,Jonathan Thomm; Aleksandar Terzic; Geethan Karunaratne; Giacomo Camposampiero; Bernhard Schölkopf; Abbas Rahimi
Representational Strengths and Limitations of Transformers,2023-11-16,http://arxiv.org/abs/2306.02896,Clayton Sanford; Daniel Hsu; Matus Telgarsky
Large Language Models Cannot Self-Correct Reasoning Yet,2023-10-13,https://openreview.net/forum?id=IkmD3fKBPQ,Jie Huang; Xinyun Chen; Swaroop Mishra; Huaixiu Steven Zheng; Adams Wei Yu; Xinying Song; Denny Zhou
Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth,2023-08-01,http://arxiv.org/abs/2103.03404,Yihe Dong; Jean-Baptiste Cordonnier; Andreas Loukas
Limits for Learning with Language Models,2023-06-21,http://arxiv.org/abs/2306.12213,Nicholas Asher; Swarnadeep Bhar; Akshay Chaturvedi; Julie Hunter; Soumya Paul
Your Transformer May Not be as Powerful as You Expect,2022-10-31,https://openreview.net/forum?id=NQFFNdsOGD,Shengjie Luo; Shanda Li; Shuxin Zheng; Tie-Yan Liu; Liwei Wang; Di He
The Devil in Linear Transformer,2022-10-19,http://arxiv.org/abs/2210.10340,Zhen Qin; XiaoDong Han; Weixuan Sun; Dongxu Li; Lingpeng Kong; Nick Barnes; Yiran Zhong
Attention Mechanisms Don't Learn Additive Models: Rethinking Feature Importance for Transformers,2024-05-22,http://arxiv.org/abs/2405.13536,Tobias Leemann; Alina Fastowski; Felix Pfeiffer; Gjergji Kasneci
Limits of Deep Learning: Sequence Modeling through the Lens of Complexity Theory,2024-05-26,http://arxiv.org/abs/2405.16674,Nikola Zubić; Federico Soldá; Aurelio Sulser; Davide Scaramuzza
Language Models Need Inductive Biases to Count Inductively,2024-05-30,http://arxiv.org/abs/2405.20131,Yingshan Chang; Yonatan Bisk
How Far Can Transformers Reason? The Locality Barrier and Inductive Scratchpad,2024-06-10,http://arxiv.org/abs/2406.06467,Emmanuel Abbe; Samy Bengio; Aryo Lotfi; Colin Sandon; Omid Saremi
Transformers Need Glasses! Information Over-squashing in Language Tasks,2024-06-06,http://arxiv.org/abs/2406.04267,Federico Barbero; Andrea Banino; Steven Kapturowski; Dharshan Kumaran; João G.M. Araújo; Alex Vitvitskyi; Razvan Pascanu; Petar Veličković
On Limitation of Transformer for Learning HMMs,2024-06-06,http://arxiv.org/abs/2406.04089,Jiachen Hu; Qinghua Liu; Chi Jin
Hopping Too Late: Exploring the Limitations of Large Language Models on Multi-Hop Queries,2024-06-18,http://arxiv.org/abs/2406.12775,Eden Biran; Daniela Gottesman; Sohee Yang
When can transformers compositionally generalize in-context?,2024-07-17,http://arxiv.org/abs/2407.12275,Seijin Kobayashi; Simon Schug; Yassir Akram; Florian Redhardt; Johannes von Oswald; Razvan Pascanu; Guillaume Lajoie; João Sacramento
When Can Transformers Count to n?,2024-07-21,http://arxiv.org/abs/2407.15160,Gilad Yehudai; Haim Kaplan; Asma Ghandeharioun; Mor Geva; Amir Globerson
Your Context Is Not an Array: Unveiling Random Access Limitations in Transformers,2024-08-10,http://arxiv.org/abs/2408.05506,MohammadReza Ebrahimi; Sunny Panchal; Roland Memisevic
One-layer transformers fail to solve the induction heads task,2024-08-26,http://arxiv.org/abs/2408.14332,Clayton Sanford; Daniel Hsu; Matus Telgarsky
Self-Attention Limits Working Memory Capacity of Transformer-Based Models,2024-09-16,http://arxiv.org/abs/2409.10715,Dongyu Gong; Hantao Zhang
How Numerical Precision Affects Mathematical Reasoning Capabilities of LLMs,2024-10-17,http://arxiv.org/abs/2410.13857,Guhao Feng; Kai Yang; Yuntian Gu; Xinyue Ai; Shengjie Luo; Jiacheng Sun; Di He; Zhenguo Li; Liwei Wang
On the Ability and Limitations of Transformers to Recognize Formal Languages,2020-09-23,http://arxiv.org/abs/2009.11264,Satwik Bhattamishra; Kabir Ahuja; Navin Goyal
Circuit Complexity Bounds for RoPE-based Transformer Architecture,2024-11-12,http://arxiv.org/abs/2411.07602,Bo Chen; Xiaoyu Li; Yingyu Liang; Jiangxuan Long; Zhenmei Shi; Zhao Song
Consistent Bidirectional Language Modelling: Expressive Power and Representational Conciseness,2024-11,http://aclanthology.org/2024.emnlp-main.328,Georgi Shopov; Stefan Gerdjikov
