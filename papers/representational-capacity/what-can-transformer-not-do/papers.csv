Title,Date,Url,Author
Collapse of Self-trained Language Models,2024-04-02,http://arxiv.org/abs/2404.02305,David Herel; Tomas Mikolov
The pitfalls of next-token prediction,2024-03-11,http://arxiv.org/abs/2403.06963,Gregor Bachmann; Vaishnavh Nagarajan
Why are Sensitive Functions Hard for Transformers?,2024-03-03,http://arxiv.org/abs/2402.09963,Michael Hahn; Mark Rofin
"Transformers are Expressive, But Are They Expressive Enough for Regression?",2024-02-23,http://arxiv.org/abs/2402.15478,Swaroop Nath; Harshad Khadilkar; Pushpak Bhattacharyya
Limits of Transformer Language Models on Learning Algorithmic Compositions,2024-02-13,http://arxiv.org/abs/2402.05785,Jonathan Thomm; Aleksandar Terzic; Geethan Karunaratne; Giacomo Camposampiero; Bernhard Sch√∂lkopf; Abbas Rahimi
Representational Strengths and Limitations of Transformers,2023-11-16,http://arxiv.org/abs/2306.02896,Clayton Sanford; Daniel Hsu; Matus Telgarsky
Large Language Models Cannot Self-Correct Reasoning Yet,2023-10-13,https://openreview.net/forum?id=IkmD3fKBPQ,Jie Huang; Xinyun Chen; Swaroop Mishra; Huaixiu Steven Zheng; Adams Wei Yu; Xinying Song; Denny Zhou
Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth,2023-08-01,http://arxiv.org/abs/2103.03404,Yihe Dong; Jean-Baptiste Cordonnier; Andreas Loukas
Limits for Learning with Language Models,2023-06-21,http://arxiv.org/abs/2306.12213,Nicholas Asher; Swarnadeep Bhar; Akshay Chaturvedi; Julie Hunter; Soumya Paul
Your Transformer May Not be as Powerful as You Expect,2022-10-31,https://openreview.net/forum?id=NQFFNdsOGD,Shengjie Luo; Shanda Li; Shuxin Zheng; Tie-Yan Liu; Liwei Wang; Di He
The Devil in Linear Transformer,2022-10-19,http://arxiv.org/abs/2210.10340,Zhen Qin; XiaoDong Han; Weixuan Sun; Dongxu Li; Lingpeng Kong; Nick Barnes; Yiran Zhong
