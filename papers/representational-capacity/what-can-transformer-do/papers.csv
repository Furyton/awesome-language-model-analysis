Title,Date,Url,Author
Transformers Can Represent $n$-gram Language Models,2024-04-23,http://arxiv.org/abs/2404.14994,Anej Svete; Ryan Cotterell
Mechanics of Next Token Prediction with Self-Attention,2024-04-18,https://proceedings.mlr.press/v238/li24f.html,Yingcong Li; Yixiao Huang; Muhammed E. Ildiz; Ankit Singh Rawat; Samet Oymak
When can transformers reason with abstract symbols?,2024-04-16,http://arxiv.org/abs/2310.09753,Enric Boix-Adsera; Omid Saremi; Emmanuel Abbe; Samy Bengio; Etai Littwin; Joshua Susskind
The Illusion of State in State-Space Models,2024-04-12,http://arxiv.org/abs/2404.08819,William Merrill; Jackson Petty; Ashish Sabharwal
Attention is Naturally Sparse with Gaussian Distributed Input,2024-04-03,http://arxiv.org/abs/2404.02690,Yichuan Deng; Zhao Song; Chiwun Yang
What Can Transformer Learn with Varying Depth? Case Studies on Sequence Learning Tasks,2024-04-01,http://arxiv.org/abs/2404.01601,Xingwu Chen; Difan Zou
The Topos of Transformer Networks,2024-03-27,http://arxiv.org/abs/2403.18415,Mattia Jacopo Villani; Peter McBurney
Simulating Weighted Automata over Sequences and Trees with Transformers,2024-03-12,http://arxiv.org/abs/2403.09728,Michael Rizvi; Maude Lizaire; Clara Lacroce; Guillaume Rabusseau
Simplicity Bias of Transformers to Learn Low Sensitivity Functions,2024-03-11,http://arxiv.org/abs/2403.06925,Bhavya Vasudeva; Deqing Fu; Tianyi Zhou; Elliott Kau; Youqi Huang; Vatsal Sharan
On the Origins of Linear Representations in Large Language Models,2024-03-06,http://arxiv.org/abs/2403.03867,Yibo Jiang; Goutham Rajendran; Pradeep Ravikumar; Bryon Aragam; Victor Veitch
How Well Can Transformers Emulate In-context Newton's Method?,2024-03-05,http://arxiv.org/abs/2403.03183,Angeliki Giannou; Liu Yang; Tianhao Wang; Dimitris Papailiopoulos; Jason D. Lee
RNNs are not Transformers (Yet): The Key Bottleneck on In-context Retrieval,2024-02-29,http://arxiv.org/abs/2402.18510,Kaiyue Wen; Xingyu Dang; Kaifeng Lyu
Implicit Bias of Next-Token Prediction,2024-02-28,http://arxiv.org/abs/2402.18551,Christos Thrampoulidis
On the Expressive Power of a Variant of the Looped Transformer,2024-02-21,http://arxiv.org/abs/2402.13572,Yihang Gao; Chuanyang Zheng; Enze Xie; Han Shi; Tianyang Hu; Yu Li; Michael K. Ng; Zhenguo Li; Zhaoqiang Liu
From Self-Attention to Markov Models: Unveiling the Dynamics of Generative Transformers,2024-02-20,http://arxiv.org/abs/2402.13512,M. Emrullah Ildiz; Yixiao Huang; Yingcong Li; Ankit Singh Rawat; Samet Oymak
Transformers Implement Functional Gradient Descent to Learn Non-Linear Functions In Context,2024-02-15,http://arxiv.org/abs/2312.06528,Xiang Cheng; Yuxin Chen; Suvrit Sra
"Compositional Capabilities of Autoregressive Transformers: A Study on Synthetic, Interpretable Tasks",2024-02-05,http://arxiv.org/abs/2311.12997,Rahul Ramesh; Ekdeep Singh Lubana; Mikail Khona; Robert P. Dick; Hidenori Tanaka
Are Transformers with One Layer Self-Attention Using Low-Rank Weight Matrices Universal Approximators?,2024-01-29,http://arxiv.org/abs/2307.14023,Tokio Kajitsuka; Issei Sato
Transformers are Multi-State RNNs,2024-01-11,http://arxiv.org/abs/2401.06104,Matanel Oren; Michael Hassid; Yossi Adi; Roy Schwartz
"How Capable Can a Transformer Become? A Study on Synthetic, Interpretable Tasks",2023-12-12,https://openreview.net/forum?id=KIhFggzePM,Rahul Ramesh; Mikail Khona; Robert P. Dick; Hidenori Tanaka; Ekdeep Singh Lubana
Transformers can optimally learn regression mixture models,2023-11-14,http://arxiv.org/abs/2311.08362,Reese Pathak; Rajat Sen; Weihao Kong; Abhimanyu Das
The Expressive Power of Low-Rank Adaptation,2023-10-26,http://arxiv.org/abs/2310.17513,Yuchen Zeng; Kangwook Lee
What Algorithms can Transformers Learn? A Study in Length Generalization,2023-10-24,http://arxiv.org/abs/2310.16028,Hattie Zhou; Arwen Bradley; Etai Littwin; Noam Razin; Omid Saremi; Josh Susskind; Samy Bengio; Preetum Nakkiran
Transformers as Support Vector Machines,2023-09-07,http://arxiv.org/abs/2308.16898,Davoud Ataee Tarzanagh; Yingcong Li; Christos Thrampoulidis; Samet Oymak
How Do Transformers Learn Topic Structure: Towards a Mechanistic Understanding,2023-06-15,https://openreview.net/forum?id=LMXgU4zrq6,Yuchen Li; Yuanzhi Li; Andrej Risteski
Tighter Bounds on the Expressivity of Transformer Encoders,2023-06-15,https://openreview.net/forum?id=XKcogevHj8,David Chiang; Peter Cholak; Anand Pillay
Fast Attention Requires Bounded Entries,2023-02-26,https://arxiv.org/abs/2302.13214v2,Josh Alman; Zhao Song
Transformers Learn Shortcuts to Automata,2023-02-01,https://openreview.net/forum?id=De4FYqjFueZ,Bingbin Liu; Jordan T. Ash; Surbhi Goel; Akshay Krishnamurthy; Cyril Zhang
Transformer Vs. MLP-Mixer: Exponential Expressive Gap For NLP Problems,2022-11-17,http://arxiv.org/abs/2208.08191,Dan Navon; Alex M. Bronstein
Small Transformers Compute Universal Metric Embeddings,2022-10-18,http://arxiv.org/abs/2209.06788,Anastasis Kratsios; Valentin Debarnot; Ivan Dokmanić
The Lipschitz Constant of Self-Attention,2021-06-09,http://arxiv.org/abs/2006.04710,Hyunjik Kim; George Papamakarios; Andriy Mnih
On Identifiability in Transformers,2020-02-07,http://arxiv.org/abs/1908.04211,Gino Brunner; Yang Liu; Damián Pascual; Oliver Richter; Massimiliano Ciaramita; Roger Wattenhofer
What Formal Languages Can Transformers Express? A Survey,2024-05-06,http://arxiv.org/abs/2311.00208,Lena Strobl; William Merrill; Gail Weiss; David Chiang; Dana Angluin
ALPINE: Unveiling the Planning Capability of Autoregressive Learning in Language Models,2024-05-15,http://arxiv.org/abs/2405.09220, Siwei Wang; Yifei Shen; Shi Feng; Haoran Sun; Shang-Hua Teng; Wei Chen
Transformers represent belief state geometry in their residual stream,2024-05-24,http://arxiv.org/abs/2405.15943,Adam S. Shai; Sarah E. Marzen; Lucas Teixeira; Alexander Gietelink Oldenziel; Paul M. Riechers
Transformers Can Do Arithmetic with the Right Embeddings,2024-05-27,http://arxiv.org/abs/2405.17399,Sean McLeish; Arpit Bansal; Alex Stein; Neel Jain; John Kirchenbauer; Brian R. Bartoldson; Bhavya Kailkhura; Abhinav Bhatele; Jonas Geiping; Avi Schwarzschild; Tom Goldstein
The Power of Hard Attention Transformers on Data Sequences: A Formal Language Theoretic Perspective,2024-05-25,http://arxiv.org/abs/2405.16166,Pascal Bergsträßer; Chris Köcher; Anthony Widjaja Lin; Georg Zetzsche
A One-Layer Decoder-Only Transformer is a Two-Layer RNN: With an Application to Certified Robustness,2024-05-27,http://arxiv.org/abs/2405.17361,Yuhao Zhang; Aws Albarghouthi; Loris D'Antoni
What Languages are Easy to Language-Model? A Perspective from Learning Probabilistic Regular Languages,2024-06-07,http://arxiv.org/abs/2406.04289,Nadav Borenstein; Anej Svete; Robin Chan; Josef Valvoda; Franz Nowak; Isabelle Augenstein; Eleanor Chodroff; Ryan Cotterell
Transformers Provably Learn Sparse Token Selection While Fully-Connected Nets Cannot,2024-06-11,http://arxiv.org/abs/2406.06893,Zixuan Wang; Stanley Wei; Daniel Hsu; Jason D. Lee
Seperations in the Representational Capabilities of Transformers and Recurrent Architectures,2024-06-13,http://arxiv.org/abs/2406.09347,Satwik Bhattamishra; Michael Hahn; Phil Blunsom; Varun Kanade
Universal Approximation Theory: The basic theory for large language models,2024-07-01,http://arxiv.org/abs/2407.00958,Wei Wang; Qing Li
Do Large Language Models Have Compositional Ability? An Investigation into Limitations and Scalability,2024-07-22,http://arxiv.org/abs/2407.15720,Zhuoyan Xu; Zhenmei Shi; Yingyu Liang
Transformers on Markov Data: Constant Depth Suffices,2024-07-25,http://arxiv.org/abs/2407.17686,Nived Rajaraman; Marco Bondaschi; Kannan Ramchandran; Michael Gastpar; Ashok Vardhan Makkuva
Can LLMs predict the convergence of Stochastic Gradient Descent?,2024-08-03,http://arxiv.org/abs/2408.01736,Oussama Zekri; Abdelhakim Benechehab; Ievgen Redko
Why Transformers are Obviously Good Models of Language,2024-08-07,http://arxiv.org/abs/2408.03855,Felix Hill
Attention is a smoothed cubic spline,2024-08-19,http://arxiv.org/abs/2408.09624,Zehua Lai; Lek-Heng Lim; Yucong Liu
Transformers As Approximations of Solomonoff Induction,2024-08-22,http://arxiv.org/abs/2408.12065,Nathan Young; Michael Witbrock
How Transformers Learn Structured Data: Insights from Hierarchical Filtering,2024-08-27,http://arxiv.org/abs/2408.15138,Jerome Garnier-Brun; Marc Mézard; Emanuele Moscato; Luca Saglietti
Learning Randomized Algorithms with Transformers,2024-08-20,http://arxiv.org/abs/2408.10818,Johannes von Oswald; Seijin Kobayashi; Yassir Akram; Angelika Steger
Attention is a smoothed cubic spline,2024-08-19,http://arxiv.org/abs/2408.09624,Zehua Lai; Lek-Heng Lim; Yucong Liu
Transformers As Approximations of Solomonoff Induction,2024-08-22,http://arxiv.org/abs/2408.12065,Nathan Young; Michael Witbrock
Implicit Geometry of Next-token Prediction: From Language Sparsity Patterns to Model Representations,2024-08-27,http://arxiv.org/abs/2408.15417,Yize Zhao; Tina Behnia; Vala Vakilian; Christos Thrampoulidis
A Law of Next-Token Prediction in Large Language Models,2024-08-24,http://arxiv.org/abs/2408.13442,Hangfeng He; Weijie J. Su
"Physics of Language Models: Part 1, Learning Hierarchical Language Structures",2024-06-02,http://arxiv.org/abs/2305.13673,Zeyuan Allen-Zhu; Yuanzhi Li