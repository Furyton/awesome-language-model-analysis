Title,Date,Url,Author
Let's Think Dot by Dot: Hidden Computation in Transformer Language Models,2024-04-24,http://arxiv.org/abs/2404.15758,Jacob Pfau; William Merrill; Samuel R. Bowman
Chain of Thought Empowers Transformers to Solve Inherently Serial Problems,2024-02-20,http://arxiv.org/abs/2402.12875,Zhiyuan Li; Hong Liu; Denny Zhou; Tengyu Ma
Towards Revealing the Mystery behind Chain of Thought: A Theoretical Perspective,2023-12-22,http://arxiv.org/abs/2305.15408,Guhao Feng; Bohang Zhang; Yuntian Gu; Haotian Ye; Di He; Liwei Wang
Why Can Large Language Models Generate Correct Chain-of-Thoughts?,2023-10-20,http://arxiv.org/abs/2310.13571,Rasul Tutunov; Antoine Grosnit; Juliusz Ziomek; Jun Wang; Haitham Bou-Ammar
How Large Language Models Implement Chain-of-Thought?,2023-10-13,https://openreview.net/forum?id=b2XfOm3RJa,Yiqun Wang; Sile Hu; Yonggang Zhang; Xiang Tian; Xuesong Liu; Yaowu Chen; Xu Shen; Jieping Ye
The Expressive Power of Transformers with Chain of Thought,2023-10-13,https://openreview.net/forum?id=NjNGlPh8Wh,William Merrill; Ashish Sabharwal
Iteration Head: A Mechanistic Study of Chain-of-Thought,2024-06-04,http://arxiv.org/abs/2406.02128,Vivien Cabannes; Charles Arnal; Wassim Bouaziz; Alice Yang; Francois Charton; Julia Kempe
On the Representational Capacity of Neural Language Models with Chain-of-Thought Reasoning,2024-06-20,http://arxiv.org/abs/2406.14197,Franz Nowak; Anej Svete; Alexandra Butoi; Ryan Cotterell
Unveiling the Statistical Foundations of Chain-of-Thought Prompting Methods,2024-08-25,http://arxiv.org/abs/2408.14511,Xinyang Hu; Fengzhuo Zhang; Siyu Chen; Zhuoran Yang