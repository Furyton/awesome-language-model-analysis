Title,Date,Url,Author
"Physics of Language Models: Part 3.3, Knowledge Capacity Scaling Laws",2024-04-08,http://arxiv.org/abs/2404.05405,Zeyuan Allen-Zhu; Yuanzhi Li
Memorization Capacity of Multi-Head Attention in Transformers,2024-03-02,http://arxiv.org/abs/2306.02010,Sadegh Mahdavi; Renjie Liao; Christos Thrampoulidis
Birth of a Transformer: A Memory Viewpoint,2023-11-06,http://arxiv.org/abs/2306.00802,Alberto Bietti; Vivien Cabannes; Diane Bouchacourt; Herve Jegou; Leon Bottou
"Physics of Language Models: Part 3.2, Knowledge Manipulation",2023-09-25,http://arxiv.org/abs/2309.14402,Zeyuan Allen-Zhu; Yuanzhi Li
Can Neural Network Memorization Be Localized?,2023-07-18,http://arxiv.org/abs/2307.09542,Pratyush Maini; Michael C. Mozer; Hanie Sedghi; Zachary C. Lipton; J. Zico Kolter; Chiyuan Zhang
A Multi-Perspective Analysis of Memorization in Large Language Models,2024-05-19,http://arxiv.org/abs/2405.11577,Bowen Chen; Namgi Han; Yusuke Miyao
Upper and lower memory capacity bounds of transformers for next-token prediction,2024-05-22,http://arxiv.org/abs/2405.13718,Liam Madden; Curtis Fox; Christos Thrampoulidis
Quantifying Memorization Across Neural Language Models,2022-02-15,http://arxiv.org/abs/2202.07646,Nicholas Carlini; Daphne Ippolito; Matthew Jagielski; Katherine Lee; Florian Tramer; Chiyuan Zhang
Knowledge Circuits in Pretrained Transformers,2024-05-28,http://arxiv.org/abs/2405.17969,Yunzhi Yao; Ningyu Zhang; Zekun Xi; Mengru Wang; Ziwen Xu; Shumin Deng; Huajun Chen
"Be like a Goldfish, Don't Memorize! Mitigating Memorization in Generative LLMs",2024-06-14,http://arxiv.org/abs/2406.10209,Abhimanyu Hans; Yuxin Wen; Neel Jain; John Kirchenbauer; Hamid Kazemi; Prajwal Singhania; Siddharth Singh; Gowthami Somepalli; Jonas Geiping; Abhinav Bhatele; Tom Goldstein
Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data,2024-06-20,http://arxiv.org/abs/2406.14546,Johannes Treutlein; Dami Choi; Jan Betley; Cem Anil; Samuel Marks; Roger Baker Grosse; Owain Evans
Scaling Laws for Fact Memorization of Large Language Models,2024-06-22,http://arxiv.org/abs/2406.15720,Xingyu Lu; Xiaonan Li; Qinyuan Cheng; Kai Ding; Xuanjing Huang; Xipeng Qiu
How Do Large Language Models Acquire Factual Knowledge During Pretraining?,2024-06-17,http://arxiv.org/abs/2406.11813,Hoyeon Chang; Jinho Park; Seonghyeon Ye; Sohee Yang; Youngkyung Seo; Du-Seong Chang; Minjoon Seo
Uncovering Latent Memories: Assessing Data Leakage and Memorization Patterns in Large Language Models,2024-06-20,http://arxiv.org/abs/2406.14549,Sunny Duan; Mikail Khona; Abhiram Iyer; Rylan Schaeffer; Ila R Fiete
Estimating Knowledge in Large Language Models Without Generating a Single Token,2024-06-18,http://arxiv.org/abs/2406.12673,Daniela Gottesman; Mor Geva
Understanding Finetuning for Factual Knowledge Extraction,2024-06-20,http://arxiv.org/abs/2406.14785,Gaurav Ghosal; Tatsunori Hashimoto; Aditi Raghunathan
Do LLMs dream of elephants (when told not to)? Latent concept association and associative memory in transformers,2024-06-26,http://arxiv.org/abs/2406.18400,Yibo Jiang; Goutham Rajendran; Pradeep Ravikumar; Bryon Aragam
Demystifying Verbatim Memorization in Large Language Models,2024-07-25,http://arxiv.org/abs/2407.17817,Jing Huang; Diyi Yang; Christopher Potts
From Internal Conflict to Contextual Adaptation of Language Models,2024-07-24,http://arxiv.org/abs/2407.17023,Sara Vera MarjanoviÄ‡; Haeun Yu; Pepa Atanasova; Maria Maistro; Christina Lioma; Isabelle Augenstein
"Generalisation First, Memorisation Second? Memorisation Localisation for Natural Language Classification Tasks",2024-08-09,http://arxiv.org/abs/2408.04965,Verna Dankers; Ivan Titov
"Great Memory, Shallow Reasoning: Limits of kNN-LMs",2024-08-21,http://arxiv.org/abs/2408.11815,Shangyi Geng; Wenting Zhao; Alexander M Rush
Memorisation In In-Context Learning,2024-08-21,http://arxiv.org/abs/2408.11546,Shahriar Golchin; Mihai Surdeanu; Steven Bethard; Eduardo Blanco; Ellen Riloff
"Understanding Memorisation in LLMs: Dynamics, Influencing Factors, and Implications",2024-07-27,http://arxiv.org/abs/2407.19262,Till Speicher; Mohammad Aflah Khan; Qinyuan Wu; Vedant Nanda; Soumi Das; Bishwamittra Ghosh; Krishna P. Gummadi; Evimaria Terzi
Generalization v.s. Memorization: Tracing Language Models' Capabilities Back to Pretraining Data,2024-07-20,http://arxiv.org/abs/2407.14985,Antonis Antoniades; Xinyi Wang; Yanai Elazar; Alfonso Amayuelas; Alon Albalak; Kexun Zhang; William Yang Wang
Induction Heads as an Essential Mechanism for Pattern Matching in In-context Learning,2024-07-09,http://arxiv.org/abs/2407.07011,J. Crosbie; E. Shutova
"Schrodingers Memory: Large Language Models",2024-09-16,https://arxiv.org/pdf/2409.10482,Wei Wang; Qing Li
Self-Attention Limits Working Memory Capacity of Transformer-Based Models,2024-09-16,http://arxiv.org/abs/2409.10715,Dongyu Gong; Hantao Zhang
"Physics of Language Models: Part 3.1, Knowledge Storage and Extraction",2024-07-16,http://arxiv.org/abs/2309.14316,Zeyuan Allen-Zhu; Yuanzhi Li
"Co-occurrence is not Factual Association in Language Models",2024-09-21,https://arxiv.org/pdf/2409.14057,Xiao Zhang; Miao Li; Ji Wu
