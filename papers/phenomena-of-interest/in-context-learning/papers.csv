Title,Date,Url,Author
What needs to go right for an induction head? A mechanistic study of in-context learning circuits and their formation,2024-04-10,http://arxiv.org/abs/2404.07129,Aaditya K. Singh; Ted Moskovitz; Felix Hill; Stephanie C. Y. Chan; Andrew M. Saxe
Is attention required for ICL? Exploring the Relationship Between Model Architecture and In-Context Learning Ability,2024-04-01,http://arxiv.org/abs/2310.08049,Ivan Lee; Nan Jiang; Taylor Berg-Kirkpatrick
"Training Dynamics of Multi-Head Softmax Attention for In-Context Learning: Emergence, Convergence, and Optimality",2024-02-29,http://arxiv.org/abs/2402.19442,Siyu Chen; Heejune Sheen; Tianhao Wang; Zhuoran Yang
How Transformers Learn Causal Structure with Gradient Descent,2024-02-22,http://arxiv.org/abs/2402.14735,Eshaan Nichani; Alex Damian; Jason D. Lee
In-Context Learning of a Linear Transformer Block: Benefits of the MLP Component and One-Step GD Initialization,2024-02-22,http://arxiv.org/abs/2402.14951,Ruiqi Zhang; Jingfeng Wu; Peter L. Bartlett
Identifying Semantic Induction Heads to Understand In-Context Learning,2024-02-20,http://arxiv.org/abs/2402.13055,Jie Ren; Qipeng Guo; Hang Yan; Dongrui Liu; Xipeng Qiu; Dahua Lin
The Transient Nature of Emergent In-Context Learning in Transformers,2023-12-11,http://arxiv.org/abs/2311.08360,Aaditya K. Singh; Stephanie C. Y. Chan; Ted Moskovitz; Erin Grant; Andrew M. Saxe; Felix Hill
In-Context Learning Functions with Varying Number of Minima,2023-11-21,http://arxiv.org/abs/2311.12538,David Oniani; Yanshan Wang
Exploring the Relationship between In-Context Learning and Instruction Tuning,2023-11-17,http://arxiv.org/abs/2311.10367,Hanyu Duan; Yixuan Tang; Yi Yang; Ahmed Abbasi; Kar Yan Tam
When does In-context Learning Fall Short and Why? A Study on Specification-Heavy Tasks,2023-11-15,http://arxiv.org/abs/2311.08993,Hao Peng; Xiaozhi Wang; Jianhui Chen; Weikai Li; Yunjia Qi; Zimu Wang; Zhili Wu; Kaisheng Zeng; Bin Xu; Lei Hou; Juanzi Li
"In-context Learning Generalizes, But Not Always Robustly: The Case of Syntax",2023-11-13,http://arxiv.org/abs/2311.07811,Aaron Mueller; Albert Webson; Jackson Petty; Tal Linzen
Transformers learn to implement preconditioned gradient descent for in-context learning,2023-11-09,http://arxiv.org/abs/2306.00297,Kwangjun Ahn; Xiang Cheng; Hadi Daneshmand; Suvrit Sra
In-Context Learning Creates Task Vectors,2023-10-24,http://arxiv.org/abs/2310.15916,Roee Hendel; Mor Geva; Amir Globerson
Function Vectors in Large Language Models,2023-10-23,http://arxiv.org/abs/2310.15213,Eric Todd; Millicent L. Li; Arnab Sen Sharma; Aaron Mueller; Byron C. Wallace; David Bau
In-context Learning with Transformer Is Really Equivalent to a Contrastive Learning Pattern,2023-10-19,http://arxiv.org/abs/2310.13220,Ruifeng Ren; Yong Liu
How Do Transformers Learn In-Context Beyond Simple Functions? A Case Study on Learning with Representations,2023-10-16,http://arxiv.org/abs/2310.10616,Tianyu Guo; Wei Hu; Song Mei; Huan Wang; Caiming Xiong; Silvio Savarese; Yu Bai
Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions,2023-10-13,https://openreview.net/forum?id=ekeyCgeRfC,Satwik Bhattamishra; Arkil Patel; Phil Blunsom; Varun Kanade
How Many Pretraining Tasks Are Needed for In-Context Learning of Linear Regression?,2023-10-13,https://openreview.net/forum?id=vSh5ePa0ph,Jingfeng Wu; Difan Zou; Zixiang Chen; Vladimir Braverman; Quanquan Gu; Peter Bartlett
In-Context Learning Learns Label Relationships but Is Not Conventional Learning,2023-10-13,https://openreview.net/forum?id=YPIA7bgd5y,Jannik Kossen; Yarin Gal; Tom Rainforth
In-context Convergence of Transformers,2023-10-13,https://openreview.net/forum?id=kxpswbhr1r,Yu Huang; Yuan Cheng; Yingbin Liang
In-Context Learning through the Bayesian Prism,2023-10-13,https://openreview.net/forum?id=HX5ujdsSon,Madhur Panwar; Kabir Ahuja; Navin Goyal
Do pretrained Transformers Really Learn In-context by Gradient Descent?,2023-10-12,http://arxiv.org/abs/2310.08540,Lingfeng Shen; Aayush Mishra; Daniel Khashabi
"What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization",2023-10-10,http://arxiv.org/abs/2305.19420,Yufeng Zhang; Fengzhuo Zhang; Zhuoran Yang; Zhaoran Wang
Explaining Emergent In-Context Learning as Kernel Regression,2023-10-05,http://arxiv.org/abs/2305.12766,Chi Han; Ziqi Wang; Han Zhao; Heng Ji
CausalLM is not optimal for in-context learning,2023-09-02,http://arxiv.org/abs/2308.06912,Nan Ding; Tomer Levinboim; Jialin Wu; Sebastian Goodman; Radu Soricut
One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention,2023-07-07,http://arxiv.org/abs/2307.03576,Arvind Mahankali; Tatsunori B. Hashimoto; Tengyu Ma
Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection,2023-07-06,http://arxiv.org/abs/2306.04637,Yu Bai; Fan Chen; Huan Wang; Caiming Xiong; Song Mei
Transformers Learn In-Context by Gradient Descent,2023-06-15,https://openreview.net/forum?id=tHvXrFQma5,Johannes Von Oswald; Eyvind Niklasson; Ettore Randazzo; Joao Sacramento; Alexander Mordvintsev; Andrey Zhmoginov; Max Vladymyrov
The Closeness of In-Context Learning and Weight Shifting for Softmax Regression,2023-04-26,http://arxiv.org/abs/2304.13276,Shuai Li; Zhao Song; Yu Xia; Tong Yu; Tianyi Zhou
A Theory of Emergent In-Context Learning as Implicit Structure Induction,2023-03-14,http://arxiv.org/abs/2303.07971,Michael Hahn; Navin Goyal
The Learnability of In-Context Learning,2023-03-14,http://arxiv.org/abs/2303.07895,Noam Wies; Yoav Levine; Amnon Shashua
What Can Transformers Learn In-Context? A Case Study of Simple Function Classes,2023-01-14,http://arxiv.org/abs/2208.01066,Shivam Garg; Dimitris Tsipras; Percy Liang; Gregory Valiant
Transformers generalize differently from information stored in context vs in weights,2022-10-13,http://arxiv.org/abs/2210.05675,Stephanie C. Y. Chan; Ishita Dasgupta; Junkyung Kim; Dharshan Kumaran; Andrew K. Lampinen; Felix Hill
Transformers Learn Higher-Order Optimization Methods for In-Context Learning: A Study with Linear Models,2023-10-26,https://arxiv.org/abs/2310.17086v1,Deqing Fu; Tian-Qi Chen; Robin Jia; Vatsal Sharan
In-Context Learning with Long-Context Models: An In-Depth Exploration,2024-04-30,http://arxiv.org/abs/2405.00200,Amanda Bertsch; Maor Ivgi; Uri Alon; Jonathan Berant; Matthew R. Gormley; Graham Neubig
An Information-Theoretic Analysis of In-Context Learning,2024-01-28,http://arxiv.org/abs/2401.15530,Hong Jun Jeon; Jason D. Lee; Qi Lei; Benjamin Van Roy
Trained Transformers Learn Linear Models In-Context,2023-10-19,http://arxiv.org/abs/2306.09927,Ruiqi Zhang; Spencer Frei; Peter L. Bartlett
MLPs Learn In-Context,2024-05-24,http://arxiv.org/abs/2405.15618,William L. Tong; Cengiz Pehlevan
Towards Better Understanding of In-Context Learning Ability from In-Context Uncertainty Quantification,2024-05-24,http://arxiv.org/abs/2405.15115,Shang Liu; Zhongze Cai; Guanting Chen; Xiaocheng Li
In-Context Learning and Induction Heads,2022-09-24,http://arxiv.org/abs/2209.11895,Catherine Olsson; Nelson Elhage; Neel Nanda; Nicholas Joseph; Nova DasSarma; Tom Henighan; Ben Mann; Amanda Askell; Yuntao Bai; Anna Chen; Tom Conerly; Dawn Drain; Deep Ganguli; Zac Hatfield-Dodds; Danny Hernandez; Scott Johnston; Andy Jones; Jackson Kernion; Liane Lovitt; Kamal Ndousse; Dario Amodei; Tom Brown; Jack Clark; Jared Kaplan; Sam McCandlish; Chris Olah
On Mesa-Optimization in Autoregressively Trained Transformers: Emergence and Capability,2024-05-27,http://arxiv.org/abs/2405.16845,Chenyu Zheng; Wei Huang; Rongzhen Wang; Guoqiang Wu; Jun Zhu; Chongxuan Li
Transformer In-Context Learning for Categorical Data,2024-05-27,http://arxiv.org/abs/2405.17248,Aaron T. Wang; Ricardo Henao; Lawrence Carin
Automatic Domain Adaptation by Transformers in In-Context Learning,2024-05-27,http://arxiv.org/abs/2405.16819,Ryuichiro Hataya; Kota Matsui; Masaaki Imaizumi
Unifying Demonstration Selection and Compression for In-Context Learning,2024-05-27,http://arxiv.org/abs/2405.17062,Jun Gao
On the Noise Robustness of In-Context Learning for Text Generation,2024-05-27,http://arxiv.org/abs/2405.17264,Hongfu Gao; Feipeng Zhang; Wenyu Jiang; Jun Shu; Feng Zheng; Hongxin Wei
Why Larger Language Models Do In-context Learning Differently?,2024-05-30,http://arxiv.org/abs/2405.19592,Zhenmei Shi; Junyi Wei; Zhuoyan Xu; Yingyu Liang
Is In-Context Learning Sufficient for Instruction Following in LLMs?,2024-05-30,http://arxiv.org/abs/2405.19874,Hao Zhao; Maksym Andriushchenko; Francesco Croce; Nicolas Flammarion
Does learning the right latent variables necessarily improve in-context learning?,2024-05-29,http://arxiv.org/abs/2405.19162,Sarthak Mittal; Eric Elmoznino; Leo Gagnon; Sangnie Bhardwaj; Dhanya Sridhar; Guillaume Lajoie
A Theory of In-Context Learning in Transformers,2024-05-29,http://arxiv.org/abs/2405.18634,Yifei Wang; Yuyang Wu; Zeming Wei; Stefanie Jegelka; Yisen Wang
BERTs are Generative In-Context Learners,2024-06-07,http://arxiv.org/abs/2406.04823,David Samuel
Enhancing In-Context Learning Performance with just SVD-Based Weight Pruning: A Theoretical Perspective,2024-06-06,http://arxiv.org/abs/2406.03768,Xinhao Yao; Xiaolin Hu; Shenzhi Yang; Yong Liu
What Do Language Models Learn in Context? The Structured Task Hypothesis,2024-06-06,http://arxiv.org/abs/2406.04216,Jiaoda Li; Yifan Hou; Mrinmaya Sachan; Ryan Cotterell
Learning to grok: Emergence of in-context learning and skill composition in modular arithmetic tasks,2024-06-04,http://arxiv.org/abs/2406.02550,Tianyu He; Darshil Doshi; Aritra Das; Andrey Gromov
Exact Conversion of In-Context Learning to Model Weights in Linearized-Attention Transformers,2024-06-05,http://arxiv.org/abs/2406.02847,Brian K Chen; Tianyang Hu; Hui Jin; Hwee Kuan Lee; Kenji Kawaguchi
Estimating the Hallucination Rate of Generative AI,2024-06-11,http://arxiv.org/abs/2406.07457,Andrew Jesson; Nicolas Beltran-Velez; Quentin Chu; Sweta Karlekar; Jannik Kossen; Yarin Gal; John P. Cunningham; David Blei
"State Soup: In-Context Skill Learning, Retrieval and Mixing",2024-06-12,http://arxiv.org/abs/2406.08423,Maciej Pióro; Maciej Wołczyk; Razvan Pascanu; Johannes von Oswald; João Sacramento
In-Context In-Context Learning with Transformer Neural Processes,2024-06-19,http://arxiv.org/abs/2406.13493,Matthew Ashman; Cristiana Diaconu; Adrian Weller; Richard E. Turner
Probing the Decision Boundaries of In-context Learning in Large Language Models,2024-06-17,http://arxiv.org/abs/2406.11233,Siyan Zhao; Tung Nguyen; Aditya Grover
When can transformers compositionally generalize in-context?,2024-07-17,http://arxiv.org/abs/2407.12275,Seijin Kobayashi; Simon Schug; Yassir Akram; Florian Redhardt; Johannes von Oswald; Razvan Pascanu; Guillaume Lajoie; João Sacramento
Can Looped Transformers Learn to Implement Multi-step Gradient Descent for In-context Learning?,2024-05-02,https://openreview.net/pdf?id=o8AaRKbP9K,Khashayar Gatmiry; Nikunj Saunshi; Sashank J. Reddi; Stefanie Jegelka; Sanjiv Kumar
Transformers learn to implement preconditioned gradient descent for in-context learning,2023-11-09,http://arxiv.org/abs/2306.00297,Kwangjun Ahn; Xiang Cheng; Hadi Daneshmand; Suvrit Sra
How do Transformers perform In-Context Autoregressive Learning?,2024-02-08,http://arxiv.org/abs/2402.05787,Michael E. Sander; Raja Giryes; Taiji Suzuki; Mathieu Blondel; Gabriel Peyré
In-context Learning on Function Classes Unveiled for Transformers,2024-05-02,https://openreview.net/pdf?id=rJkGOARXns,Zhijie Wang; Bo Jiang; Shuai Li
Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks,2024-02-06,http://arxiv.org/abs/2402.04248,Jongho Park; Jaeseung Park; Zheyang Xiong; Nayoung Lee; Jaewoong Cho; Samet Oymak; Kangwook Lee; Dimitris Papailiopoulos
In-Context Learning with Representations: Contextual Generalization of Trained Transformers,2024-08-19,http://arxiv.org/abs/2408.10147,Tong Yang; Yu Huang; Yingbin Liang; Yuejie Chi
How Transformers Utilize Multi-Head Attention in In-Context Learning? A Case Study on Sparse Linear Regression,2024-08-08,http://arxiv.org/abs/2408.04532,Xingwu Chen; Lei Zhao; Difan Zou
Transformers are Universal In-context Learners,2024-08-02,http://arxiv.org/abs/2408.01367,Takashi Furuya; Maarten V. de Hoop; Gabriel Peyré
Transformers are Minimax Optimal Nonparametric In-Context Learners,2024-08-22,http://arxiv.org/abs/2408.12186,Juno Kim; Tai Nakamaki; Taiji Suzuki
Fast Training Dataset Attribution via In-Context Learning,2024-08-14,http://arxiv.org/abs/2408.11852,Milad Fotouhi; Mohammad Taha Bahadori; Oluwaseyi Feyisetan; Payman Arabshahi; David Heckerman
Memorisation In In-Context Learning,2024-08-21,http://arxiv.org/abs/2408.11546,Shahriar Golchin; Mihai Surdeanu; Steven Bethard; Eduardo Blanco; Ellen Riloff
In-Context Learning with Representations: Contextual Generalization of Trained Transformers,2024-08-19,http://arxiv.org/abs/2408.10147,Tong Yang; Yu Huang; Yingbin Liang; Yuejie Chi
Unveiling In-Context Learning: A Coordinate System to Understand Its Working Mechanism,2024-07-24,http://arxiv.org/abs/2407.17011,Anhao Zhao; Fanghua Ye; Jinlan Fu; Xiaoyu Shen
Polynomial Regression as a Task for Understanding In-context Learning Through Finetuning and Alignment,2024-07-27,http://arxiv.org/abs/2407.19346,Max Wilcoxson; Morten Svendgård; Ria Doshi; Dylan Davis; Reya Vir; Anant Sahai
One-Layer Transformer Provably Learns One-Nearest Neighbor In Context,2024-07-24,https://klusowski.princeton.edu/sites/g/files/toruqf5901/files/documents/li2024one.pdf,Zihao Li; Yuan Cao; Cheng Gao; Yihan He; Han Liu; Jason M. Klusowski; Jianqing Fan; Mengdi Wang
Learning vs Retrieval: The Role of In-Context Examples in Regression with LLMs,2024-09-06,http://arxiv.org/abs/2409.04318,Aliakbar Nafar; Kristen Brent Venable; Parisa Kordjamshidi
Unveiling Induction Heads: Provable Training Dynamics and Feature Learning in Transformers,2024-09-10,http://arxiv.org/abs/2409.10559,Siyu Chen; Heejune Sheen; Tianhao Wang; Zhuoran Yang
Context-Scaling versus Task-Scaling in In-Context Learning,2024-10-16,http://arxiv.org/abs/2410.12783,Amirhesam Abedsoltan; Adityanarayanan Radhakrishnan; Jingfeng Wu; Mikhail Belkin
In-context learning and Occam's razor,2024-10-17,http://arxiv.org/abs/2410.14086,Eric Elmoznino; Tom Marty; Tejas Kasetty; Leo Gagnon; Sarthak Mittal; Mahan Fathi; Dhanya Sridhar; Guillaume Lajoie
Provable In-context Learning for Mixture of Linear Regressions using Transformers,2024-10-18,http://arxiv.org/abs/2410.14183,Yanhao Jin; Krishnakumar Balasubramanian; Lifeng Lai
Trained Transformer Classifiers Generalize and Exhibit Benign Overfitting In-Context,2024-10-02,http://arxiv.org/abs/2410.01774,Spencer Frei; Gal Vardi
Transformers learn variable-order Markov chains in-context,2024-10-07,http://arxiv.org/abs/2410.05493,Ruida Zhou; Chao Tian; Suhas Diggavi
Transformers Handle Endogeneity in In-Context Linear Regression,2024-10-02,http://arxiv.org/abs/2410.01265,Haodong Liang; Krishnakumar Balasubramanian; Lifeng Lai
Revisiting In-context Learning Inference Circuit in Large Language Models,2024-10-06,http://arxiv.org/abs/2410.04468,Hakaze Cho; Mariko Kato; Yoshihiro Sakai; Naoya Inoue
Bypassing the Exponential Dependency: Looped Transformers Efficiently Learn In-context by Multi-step Gradient Descent,2024-10-15,http://arxiv.org/abs/2410.11268,Bo Chen; Xiaoyu Li; Yingyu Liang; Zhenmei Shi; Zhao Song
How Transformers Implement Induction Heads: Approximation and Optimization Analysis,2024-10-15,http://arxiv.org/abs/2410.11474,Mingze Wang; Ruoxi Yu; Weinan E; Lei Wu
On the Training Convergence of Transformers for In-Context Classification,2024-10-15,http://arxiv.org/abs/2410.11475,Wei Shen; Ruida Zhou; Jing Yang; Cong Shen