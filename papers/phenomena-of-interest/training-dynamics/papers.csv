Title,Date,Url,Author
Control Theoretic Approach to Fine-Tuning and Transfer Learning,2024-04-16,http://arxiv.org/abs/2404.11013,Erkan Bayram; Shenyu Liu; Mohamed-Ali Belabbas; Tamer Başar
Look at the Text: Instruction-Tuned Language Models are More Robust Multiple Choice Selectors than You Think,2024-04-12,http://arxiv.org/abs/2404.08382,Xinpeng Wang; Chengzhi Hu; Bolei Ma; Paul Röttger; Barbara Plank
On Training Data Influence of GPT Models,2024-04-11,http://arxiv.org/abs/2404.07840,Qingyi Liu; Yekun Chai; Shuohuan Wang; Yu Sun; Keze Wang; Hua Wu
Best Practices and Lessons Learned on Synthetic Data for Language Models,2024-04-11,http://arxiv.org/abs/2404.07503,Ruibo Liu; Jerry Wei; Fangyu Liu; Chenglei Si; Yanzhe Zhang; Jinmeng Rao; Steven Zheng; Daiyi Peng; Diyi Yang; Denny Zhou; Andrew M. Dai
How Bad is Training on Synthetic Data? A Statistical Analysis of Language Model Collapse,2024-04-07,http://arxiv.org/abs/2404.05090,Mohamed El Amine Seddik; Suei-Wen Chen; Soufiane Hayou; Pierre Youssef; Merouane Debbah
Unveiling the Generalization Power of Fine-Tuned Large Language Models,2024-03-14,http://arxiv.org/abs/2403.09162,Haoran Yang; Yumeng Zhang; Jiaqi Xu; Hongyuan Lu; Pheng Ann Heng; Wai Lam
Transformers Get Stable: An End-to-End Signal Propagation Theory for Language Models,2024-03-14,http://arxiv.org/abs/2403.09635,Akhil Kedia; Mohd Abbas Zaidi; Sushil Khyalia; Jungho Jung; Harshith Goka; Haejun Lee
"Hallmarks of Optimization Trajectories in Neural Networks and LLMs: The Lengths, Bends, and Dead Ends",2024-03-12,http://arxiv.org/abs/2403.07379,Sidak Pal Singh; Bobby He; Thomas Hofmann; Bernhard Schölkopf
The Heuristic Core: Understanding Subnetwork Generalization in Pretrained Language Models,2024-03-06,http://arxiv.org/abs/2403.03942,Adithya Bhaskar; Dan Friedman; Danqi Chen
"Training Dynamics of Multi-Head Softmax Attention for In-Context Learning: Emergence, Convergence, and Optimality",2024-02-29,http://arxiv.org/abs/2402.19442,Siyu Chen; Heejune Sheen; Tianhao Wang; Zhuoran Yang
How Transformers Learn Causal Structure with Gradient Descent,2024-02-22,http://arxiv.org/abs/2402.14735,Eshaan Nichani; Alex Damian; Jason D. Lee
Transformers learn through gradual rank increase,2023-12-10,http://arxiv.org/abs/2306.07042,Enric Boix-Adsera; Etai Littwin; Emmanuel Abbe; Samy Bengio; Joshua Susskind
Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks,2023-11-21,http://arxiv.org/abs/2311.12786,Samyak Jain; Robert Kirk; Ekdeep Singh Lubana; Robert P. Dick; Hidenori Tanaka; Edward Grefenstette; Tim Rocktäschel; David Scott Krueger
Connecting Pre-trained Language Model and Downstream Task via Properties of Representation,2023-11-02,https://openreview.net/forum?id=YLOJ4aKAka,Chenwei Wu; Holden Lee; Rong Ge
Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer,2023-07-02,http://arxiv.org/abs/2305.16380,Yuandong Tian; Yiping Wang; Beidi Chen; Simon Du
A Kernel-Based View of Language Model Fine-Tuning,2023-06-15,https://openreview.net/forum?id=49dTFIGdx8,Sadhika Malladi; Alexander Wettig; Dingli Yu; Danqi Chen; Sanjeev Arora
A Stability Analysis of Fine-Tuning a Pre-Trained Model,2023-01-24,https://arxiv.org/abs/2301.09820v2,Zihao Fu; Anthony Man-Cho So; Nigel Collier
Linear Attention is (Maybe) All You Need (to Understand Transformer Optimization),2024-03-13,http://arxiv.org/abs/2310.01082,Kwangjun Ahn; Xiang Cheng; Minhak Song; Chulhee Yun; Ali Jadbabaie; Suvrit Sra
Towards a Theoretical Understanding of the 'Reversal Curse' via Training Dynamics,2024-05-07,http://arxiv.org/abs/2405.04669,Hanlin Zhu; Baihe Huang; Shaolun Zhang; Michael Jordan; Jiantao Jiao; Yuandong Tian; Stuart Russell
LoRA Training in the NTK Regime has No Spurious Local Minima,2024-02-19,http://arxiv.org/abs/2402.11867,Uijeong Jang; Jason D. Lee; Ernest K. Ryu
Infinite Limits of Multi-head Transformer Dynamics,2024-05-24,http://arxiv.org/abs/2405.15712,Blake Bordelon; Hamza Tahir Chaudhry; Cengiz Pehlevan
Understanding Linear Probing then Fine-tuning Language Models from NTK Perspective,2024-05-27,http://arxiv.org/abs/2405.16747,Akiyoshi Tomihari; Issei Sato
Zero-Shot Generalization during Instruction Tuning: Insights from Similarity and Granularity,2024-06-17,http://arxiv.org/abs/2406.11721,Bingxiang He; Ning Ding; Cheng Qian; Jia Deng; Ganqu Cui; Lifan Yuan; Huan-ang Gao; Huimin Chen; Zhiyuan Liu; Maosong Sun
Deconstructing What Makes a Good Optimizer for Language Models,2024-07-10,http://arxiv.org/abs/2407.07972,Rosie Zhao; Depen Morwani; David Brandfonbrener; Nikhil Vyas; Sham Kakade
On the Emergence of Cross-Task Linearity in the Pretraining-Finetuning Paradigm,2024-02-06,http://arxiv.org/abs/2402.03660,Zhanpeng Zhou; Zijun Chen; Yilan Chen; Bo Zhang; Junchi Yan
Clustering and Alignment: Understanding the Training Dynamics in Modular Addition,2024-08-18,http://arxiv.org/abs/2408.09414,Tiberiu Musat
Learning Dynamics of LLM Finetuning,2024-07-15,http://arxiv.org/abs/2407.10490,Yi Ren; Danica J. Sutherland
Parameter-Efficient Fine-Tuning for Continual Learning: A Neural Tangent Kernel Perspective,2024-07-24,http://arxiv.org/abs/2407.17120,Jingren Liu; Zhong Ji; YunLong Yu; Jiale Cao; Yanwei Pang; Jungong Han; Xuelong Li
Global Convergence in Training Large-Scale Transformers,2024-08,https://klusowski.princeton.edu/sites/g/files/toruqf5901/files/documents/gao2024global.pdf,Cheng Gao; Yuan Cao; Zihao Li; Yihan He; Mengdi Wang; Han Liu; Jason M. Klusowski; Jianqing Fan
On the Convergence of Encoder-only Shallow Transformers,2024-08,https://proceedings.neurips.cc/paper_files/paper/2023/file/a3cf318fbeec1126da21e9185ae9908c-Paper-Conference.pdf,Yongtao Wu; Fanghui Liu; Grigorios G Chrysos; Volkan Cevher
"The AdEMAMix Optimizer: Better, Faster, Older",2024-09-05,http://arxiv.org/abs/2409.03137,Matteo Pagliardini; Pierre Ablin; David Grangier
Optimization Hyper-parameter Laws for Large Language Models,2024-09-07,http://arxiv.org/abs/2409.04777,Xingyu Xie; Kuangyu Ding; Shuicheng Yan; Kim-Chuan Toh; Tianwen Wei
Non-asymptotic Convergence of Training Transformers for Next-token Prediction,2024-09-25,http://arxiv.org/abs/2409.17335,Ruiquan Huang; Yingbin Liang; Jing Yang
Benigh or Not-Benign Overfitting in Token Selection of Attention Mechanism,2024-09-26,http://arxiv.org/abs/2409.17625,Keitaro Sakamoto; Issei Sato
Training Nonlinear Transformers for Chain-of-Thought Inference: A Theoretical Generalization Analysis,2024-10-03,http://arxiv.org/abs/2410.02167,Hongkang Li; Meng Wang; Songtao Lu; Xiaodong Cui; Pin-Yu Chen
Trained Transformer Classifiers Generalize and Exhibit Benign Overfitting In-Context,2024-10-02,http://arxiv.org/abs/2410.01774,Spencer Frei; Gal Vardi
Theoretical Insights into Fine-Tuning Attention Mechanism: Generalization and Optimization,2024-10-03,http://arxiv.org/abs/2410.02247,Xinhao Yao; Hongjin Qian; Xiaolin Hu; Gengze Xu; Yong Liu
Investigating the Impact of Model Complexity in Large Language Models,2024-10-01,http://arxiv.org/abs/2410.00699,Jing Luo; Huiyuan Wang; Weiran Huang
Towards a Theoretical Understanding of Synthetic Data in LLM Post-Training: A Reverse-Bottleneck Perspective,2024-10-02,http://arxiv.org/abs/2410.01720,Zeyu Gan; Yong Liu
On the Optimization and Generalization of Two-layer Transformers with Sign Gradient Descent,2024-10-07,http://arxiv.org/abs/2410.04870,Bingrui Li; Wei Huang; Andi Han; Zhanpeng Zhou; Taiji Suzuki; Jun Zhu; Jianfei Chen
Adaptation Odyssey in LLMs: Why Does Additional Pretraining Sometimes Fail to Improve?,2024-10-08,http://arxiv.org/abs/2410.05581,Fırat Öncel; Matthias Bethge; Beyza Ermis; Mirco Ravanelli; Cem Subakan; Çağatay Yıldız
Understanding Warmup-Stable-Decay Learning Rates: A River Valley Loss Landscape Perspective,2024-10-07,http://arxiv.org/abs/2410.05192,Kaiyue Wen; Zhiyuan Li; Jason Wang; David Hall; Percy Liang; Tengyu Ma
How Transformers Implement Induction Heads: Approximation and Optimization Analysis,2024-10-15,http://arxiv.org/abs/2410.11474,Mingze Wang; Ruoxi Yu; Weinan E; Lei Wu
What Does It Mean to Be a Transformer? Insights from a Theoretical Hessian Analysis,2024-10-14,http://arxiv.org/abs/2410.10986,Weronika Ormaniec; Felix Dangel; Sidak Pal Singh
A distributional simplicity bias in the learning dynamics of transformers,2024-10-25,http://arxiv.org/abs/2410.19637,Riccardo Rende; Federica Gerace; Alessandro Laio; Sebastian Goldt
LoRA vs Full Fine-tuning: An Illusion of Equivalence,2024-10-28,http://arxiv.org/abs/2410.21228,Reece Shuttleworth; Jacob Andreas; Antonio Torralba; Pratyusha Sharma
Abrupt Learning in Transformers: A Case Study on Matrix Completion,2024-10-29,http://arxiv.org/abs/2410.22244,Pulkit Gopalani; Ekdeep Singh Lubana; Wei Hu
Global Convergence in Training Large-Scale Transformers,2024-10-31,http://arxiv.org/abs/2410.23610,Cheng Gao; Yuan Cao; Zihao Li; Yihan He; Mengdi Wang; Han Liu; Jason Matthew Klusowski; Jianqing Fan
What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking: A Gradient Perspective,2024-10-31,http://arxiv.org/abs/2410.23743,Ming Li; Yanhong Li; Tianyi Zhou
Learning and Transferring Sparse Contextual Bigrams with Linear Transformers,2024-10-30,http://arxiv.org/abs/2410.23438,Yunwei Ren; Zixuan Wang; Jason D. Lee