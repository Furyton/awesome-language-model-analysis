Title,Date,Url,Author
Mechanisms of non-factual hallucinations in language models,2024-03-26,http://arxiv.org/abs/2403.18167,Lei Yu; Meng Cao; Jackie Chi Kit Cheung; Yue Dong
Unfamiliar Finetuning Examples Control How Language Models Hallucinate,2024-03-08,http://arxiv.org/abs/2403.05612,Katie Kang; Eric Wallace; Claire Tomlin; Aviral Kumar; Sergey Levine
In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation,2024-03-05,http://arxiv.org/abs/2403.01548,Shiqi Chen; Miao Xiong; Junteng Liu; Zhengxuan Wu; Teng Xiao; Siyang Gao; Junxian He
Calibrated Language Models Must Hallucinate,2023-11-24,http://arxiv.org/abs/2311.14648,Adam Tauman Kalai; Santosh S. Vempala
The Curious Case of Hallucinatory Unanswerablity: Finding Truths in the Hidden States of Over-Confident Large Language Models,2023-10-18,http://arxiv.org/abs/2310.11877,Aviv Slobodkin; Omer Goldman; Avi Caciularu; Ido Dagan; Shauli Ravfogel
Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?,2024-05-09,http://arxiv.org/abs/2405.05904,Zorik Gekhman; Gal Yona; Roee Aharoni; Matan Eyal; Amir Feder; Roi Reichart; Jonathan Herzig
Estimating the Hallucination Rate of Generative AI,2024-06-11,http://arxiv.org/abs/2406.07457,Andrew Jesson; Nicolas Beltran-Velez; Quentin Chu; Sweta Karlekar; Jannik Kossen; Yarin Gal; John P. Cunningham; David Blei
Shared Imagination: LLMs Hallucinate Alike,2024-07-23,http://arxiv.org/abs/2407.16604,Yilun Zhou; Caiming Xiong; Silvio Savarese; Chien-Sheng Wu
"Large Language Models are Pattern Matchers: Editing Semi-Structured and Structured Documents with ChatGPT",2024-09-12,https://arxiv.org/pdf/2409.07732,Irene Weber
