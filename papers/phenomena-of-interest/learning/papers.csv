Title,Date,Url,Author
Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks,2023-11-21,http://arxiv.org/abs/2311.12786,Samyak Jain; Robert Kirk; Ekdeep Singh Lubana; Robert P. Dick; Hidenori Tanaka; Edward Grefenstette; Tim Rockt√§schel; David Scott Krueger
The Impact of Depth and Width on Transformer Language Model Generalization,2023-10-30,http://arxiv.org/abs/2310.19956,Jackson Petty; Sjoerd van Steenkiste; Ishita Dasgupta; Fei Sha; Dan Garrette; Tal Linzen
How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition,2023-10-09,http://arxiv.org/abs/2310.05492,Guanting Dong; Hongyi Yuan; Keming Lu; Chengpeng Li; Mingfeng Xue; Dayiheng Liu; Wei Wang; Zheng Yuan; Chang Zhou; Jingren Zhou
A Theory for Emergence of Complex Skills in Language Models,2023-07-29,http://arxiv.org/abs/2307.15936,Sanjeev Arora; Anirudh Goyal
On the Power of Foundation Models,2023-07-03,https://proceedings.mlr.press/v202/yuan23b.html,Yang Yuan
Task-Specific Skill Localization in Fine-tuned Language Models,2023-06-15,https://openreview.net/forum?id=Rgnaj43Pk0,Abhishek Panigrahi; Nikunj Saunshi; Haoyu Zhao; Sanjeev Arora
Towards Understanding Why Mask-Reconstruction Pretraining Helps in Downstream Tasks,2023-02-11,http://arxiv.org/abs/2206.03826,Jiachun Pan; Pan Zhou; Shuicheng Yan
"Same Pre-training Loss, Better Downstream: Implicit Bias Matters for Language Models",2022-10-25,http://arxiv.org/abs/2210.14199,Hong Liu; Sang Michael Xie; Zhiyuan Li; Tengyu Ma
Why Do Pretrained Language Models Help in Downstream Tasks? An Analysis of Head and Prompt Tuning,2022-04-20,http://arxiv.org/abs/2106.09226,Colin Wei; Sang Michael Xie; Tengyu Ma
A Mathematical Exploration of Why Language Models Help Solve Downstream Tasks,2021-04-14,http://arxiv.org/abs/2010.03648,Nikunj Saunshi; Sadhika Malladi; Sanjeev Arora
Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning,2020-12-22,http://arxiv.org/abs/2012.13255,Armen Aghajanyan; Luke Zettlemoyer; Sonal Gupta
How fine can fine-tuning be? Learning efficient language models,2020-06-03,https://proceedings.mlr.press/v108/radiya-dixit20a.html,Evani Radiya-Dixit; Xin Wang
Initialization is Critical to Whether Transformers Fit Composite Functions by Inference or Memorizing,2024-05-08,http://arxiv.org/abs/2405.05409,Zhongwang Zhang; Pengxiao Lin; Zhiwei Wang; Yaoyu Zhang; Zhi-Qin John Xu
Provably learning a multi-head attention layer,2024-02-06,http://arxiv.org/abs/2402.04084,Sitan Chen; Yuanzhi Li
On the Optimization and Generalization of Multi-head Attention,2023-10-19,http://arxiv.org/abs/2310.12680,Puneesh Deora; Rouzbeh Ghaderi; Hossein Taheri; Christos Thrampoulidis
Quantifying the Gain in Weak-to-Strong Generalization,2024-05-24,http://arxiv.org/abs/2405.15116,Moses Charikar; Chirag Pabbaraju; Kirankumar Shiragur
Towards Understanding How Transformer Perform Multi-step Reasoning with Matching Operation,2024-05-24,http://arxiv.org/abs/2405.15302,Zhiwei Wang; Yunji Wang; Zhongwang Zhang; Zhangchen Zhou; Hui Jin; Tianyang Hu; Jiacheng Sun; Zhenguo Li; Yaoyu Zhang; Zhi-Qin John Xu
When can transformers reason with abstract symbols?,2024-04-16,http://arxiv.org/abs/2310.09753,Enric Boix-Adsera; Omid Saremi; Emmanuel Abbe; Samy Bengio; Etai Littwin; Joshua Susskind
Large Language Models Cannot Self-Correct Reasoning Yet,2023-10-13,https://openreview.net/forum?id=IkmD3fKBPQ,Jie Huang; Xinyun Chen; Swaroop Mishra; Huaixiu Steven Zheng; Adams Wei Yu; Xinying Song; Denny Zhou
A Mechanistic Analysis of a Transformer Trained on a Symbolic Multi-Step Reasoning Task,2024-02-19,http://arxiv.org/abs/2402.11917,Jannik Brinkmann; Abhay Sheshadri; Victor Levoso; Paul Swoboda; Christian Bartelt
A statistical framework for weak-to-strong generalization,2024-05-25,http://arxiv.org/abs/2405.16236,Seamus Somerstep; Felipe Maia Polo; Moulinath Banerjee; Ya'acov Ritov; Mikhail Yurochkin; Yuekai Sun
Reality Only Happens Once: Single-Path Generalization Bounds for Transformers,2024-05-26,http://arxiv.org/abs/2405.16563,Yannick Limmer; Anastasis Kratsios; Xuwei Yang; Raeid Saqur; Blanka Horvath
Theoretical Analysis of Weak-to-Strong Generalization,2024-05-25,http://arxiv.org/abs/2405.16043,Hunter Lang; David Sontag; Aravindan Vijayaraghavan
Understanding Transformer Reasoning Capabilities via Graph Algorithms,2024-05-28,http://arxiv.org/abs/2405.18512,Clayton Sanford; Bahare Fatemi; Ethan Hall; Anton Tsitsulin; Mehran Kazemi; Jonathan Halcrow; Bryan Perozzi; Vahab Mirrokni
Linguistic Collapse: Neural Collapse in (Large) Language Models,2024-05-28,https://arxiv.org/abs/2405.17767,Robert Wu; Vardan Papyan