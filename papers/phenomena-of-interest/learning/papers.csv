Title,Date,Url,Author
Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks,2023-11-21,http://arxiv.org/abs/2311.12786,Samyak Jain; Robert Kirk; Ekdeep Singh Lubana; Robert P. Dick; Hidenori Tanaka; Edward Grefenstette; Tim Rocktäschel; David Scott Krueger
The Impact of Depth and Width on Transformer Language Model Generalization,2023-10-30,http://arxiv.org/abs/2310.19956,Jackson Petty; Sjoerd van Steenkiste; Ishita Dasgupta; Fei Sha; Dan Garrette; Tal Linzen
How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition,2023-10-09,http://arxiv.org/abs/2310.05492,Guanting Dong; Hongyi Yuan; Keming Lu; Chengpeng Li; Mingfeng Xue; Dayiheng Liu; Wei Wang; Zheng Yuan; Chang Zhou; Jingren Zhou
A Theory for Emergence of Complex Skills in Language Models,2023-07-29,http://arxiv.org/abs/2307.15936,Sanjeev Arora; Anirudh Goyal
On the Power of Foundation Models,2023-07-03,https://proceedings.mlr.press/v202/yuan23b.html,Yang Yuan
Task-Specific Skill Localization in Fine-tuned Language Models,2023-06-15,https://openreview.net/forum?id=Rgnaj43Pk0,Abhishek Panigrahi; Nikunj Saunshi; Haoyu Zhao; Sanjeev Arora
Towards Understanding Why Mask-Reconstruction Pretraining Helps in Downstream Tasks,2023-02-11,http://arxiv.org/abs/2206.03826,Jiachun Pan; Pan Zhou; Shuicheng Yan
"Same Pre-training Loss, Better Downstream: Implicit Bias Matters for Language Models",2022-10-25,http://arxiv.org/abs/2210.14199,Hong Liu; Sang Michael Xie; Zhiyuan Li; Tengyu Ma
Why Do Pretrained Language Models Help in Downstream Tasks? An Analysis of Head and Prompt Tuning,2022-04-20,http://arxiv.org/abs/2106.09226,Colin Wei; Sang Michael Xie; Tengyu Ma
A Mathematical Exploration of Why Language Models Help Solve Downstream Tasks,2021-04-14,http://arxiv.org/abs/2010.03648,Nikunj Saunshi; Sadhika Malladi; Sanjeev Arora
Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning,2020-12-22,http://arxiv.org/abs/2012.13255,Armen Aghajanyan; Luke Zettlemoyer; Sonal Gupta
How fine can fine-tuning be? Learning efficient language models,2020-06-03,https://proceedings.mlr.press/v108/radiya-dixit20a.html,Evani Radiya-Dixit; Xin Wang
Initialization is Critical to Whether Transformers Fit Composite Functions by Inference or Memorizing,2024-05-08,http://arxiv.org/abs/2405.05409,Zhongwang Zhang; Pengxiao Lin; Zhiwei Wang; Yaoyu Zhang; Zhi-Qin John Xu
Provably learning a multi-head attention layer,2024-02-06,http://arxiv.org/abs/2402.04084,Sitan Chen; Yuanzhi Li
On the Optimization and Generalization of Multi-head Attention,2023-10-19,http://arxiv.org/abs/2310.12680,Puneesh Deora; Rouzbeh Ghaderi; Hossein Taheri; Christos Thrampoulidis
Quantifying the Gain in Weak-to-Strong Generalization,2024-05-24,http://arxiv.org/abs/2405.15116,Moses Charikar; Chirag Pabbaraju; Kirankumar Shiragur
Towards Understanding How Transformer Perform Multi-step Reasoning with Matching Operation,2024-05-24,http://arxiv.org/abs/2405.15302,Zhiwei Wang; Yunji Wang; Zhongwang Zhang; Zhangchen Zhou; Hui Jin; Tianyang Hu; Jiacheng Sun; Zhenguo Li; Yaoyu Zhang; Zhi-Qin John Xu
When can transformers reason with abstract symbols?,2024-04-16,http://arxiv.org/abs/2310.09753,Enric Boix-Adsera; Omid Saremi; Emmanuel Abbe; Samy Bengio; Etai Littwin; Joshua Susskind
Large Language Models Cannot Self-Correct Reasoning Yet,2023-10-13,https://openreview.net/forum?id=IkmD3fKBPQ,Jie Huang; Xinyun Chen; Swaroop Mishra; Huaixiu Steven Zheng; Adams Wei Yu; Xinying Song; Denny Zhou
A Mechanistic Analysis of a Transformer Trained on a Symbolic Multi-Step Reasoning Task,2024-02-19,http://arxiv.org/abs/2402.11917,Jannik Brinkmann; Abhay Sheshadri; Victor Levoso; Paul Swoboda; Christian Bartelt
A statistical framework for weak-to-strong generalization,2024-05-25,http://arxiv.org/abs/2405.16236,Seamus Somerstep; Felipe Maia Polo; Moulinath Banerjee; Ya'acov Ritov; Mikhail Yurochkin; Yuekai Sun
Reality Only Happens Once: Single-Path Generalization Bounds for Transformers,2024-05-26,http://arxiv.org/abs/2405.16563,Yannick Limmer; Anastasis Kratsios; Xuwei Yang; Raeid Saqur; Blanka Horvath
Theoretical Analysis of Weak-to-Strong Generalization,2024-05-25,http://arxiv.org/abs/2405.16043,Hunter Lang; David Sontag; Aravindan Vijayaraghavan
Understanding Transformer Reasoning Capabilities via Graph Algorithms,2024-05-28,http://arxiv.org/abs/2405.18512,Clayton Sanford; Bahare Fatemi; Ethan Hall; Anton Tsitsulin; Mehran Kazemi; Jonathan Halcrow; Bryan Perozzi; Vahab Mirrokni
Linguistic Collapse: Neural Collapse in (Large) Language Models,2024-05-28,https://arxiv.org/abs/2405.17767,Robert Wu; Vardan Papyan
How Truncating Weights Improves Reasoning in Language Models,2024-06-05,http://arxiv.org/abs/2406.03068,Lei Chen; Joan Bruna; Alberto Bietti
Learning on Transformers is Provable Low-Rank and Sparse: A One-layer Analysis,2024-06-24,http://arxiv.org/abs/2406.17167,Hongkang Li; Meng Wang; Shuai Zhang; Sijia Liu; Pin-Yu Chen
Implicit meta-learning may lead language models to trust more reliable sources,2023-10-23,http://arxiv.org/abs/2310.15047,Dmitrii Krasheninnikov; Egor Krasheninnikov; Bruno Mlodozeniec; Tegan Maharaj; David Krueger
On Initialization of Transformers with Pre-trained Embeddings,2024-07-17,http://arxiv.org/abs/2407.12514,Ha Young Kim; Niranjan Balasubramanian; Byungkon Kang
Unlocking Tokens as Data Points for Generalization Bounds on Larger Language Models,2024-07-25,http://arxiv.org/abs/2407.18158,Sanae Lotfi; Yilun Kuang; Brandon Amos; Micah Goldblum; Marc Finzi; Andrew Gordon Wilson
When can transformers compositionally generalize in-context?,2024-07-17,http://arxiv.org/abs/2407.12275,Seijin Kobayashi; Simon Schug; Yassir Akram; Florian Redhardt; Johannes von Oswald; Razvan Pascanu; Guillaume Lajoie; João Sacramento
Inductive or Deductive? Rethinking the Fundamental Reasoning Abilities of LLMs,2024-07-31,http://arxiv.org/abs/2408.00114,Kewei Cheng; Jingfeng Yang; Haoming Jiang; Zhengyang Wang; Binxuan Huang; Ruirui Li; Shiyang Li; Zheng Li; Yifan Gao; Xian Li; Bing Yin; Yizhou Sun
Out-of-distribution generalization via composition: a lens through induction heads in Transformers,2024-08-18,http://arxiv.org/abs/2408.09503,Jiajun Song; Zhuoyan Xu; Yiqiao Zhong
On the Generalization of Preference Learning with DPO,2024-08-06,http://arxiv.org/abs/2408.03459,Shawn Im; Yixuan Li
"Physics of Language Models: Part 2.1, Grade-School Math and the Hidden Reasoning Process",2024-07-29,http://arxiv.org/abs/2407.13123,Tian Ye; Zicheng Xu; Yuanzhi Li; Zeyuan Allen-Zhu
Reasoning in Large Language Models: A Geometric Perspective,2024-07-02,http://arxiv.org/abs/2407.02678,Romain Cosentino; Sarath Shekkizhar
"On the Limited Generalization Capability of the Implicit Reward Model Induced by Direct Preference Optimization",2024-09-05,https://arxiv.org/pdf/2409.03650,Yong Lin, Skyler Seto, Maartje ter Hoeve, Katherine Metcalf, Barry-John Theobald, Xuan Wang, Yizhe Zhang, Chen Huang, Tong Zhang
