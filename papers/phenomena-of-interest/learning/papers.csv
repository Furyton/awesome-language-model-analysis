Title,Date,Url,Author
Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks,2023-11-21,http://arxiv.org/abs/2311.12786,Samyak Jain; Robert Kirk; Ekdeep Singh Lubana; Robert P. Dick; Hidenori Tanaka; Edward Grefenstette; Tim Rocktäschel; David Scott Krueger
The Impact of Depth and Width on Transformer Language Model Generalization,2023-10-30,http://arxiv.org/abs/2310.19956,Jackson Petty; Sjoerd van Steenkiste; Ishita Dasgupta; Fei Sha; Dan Garrette; Tal Linzen
How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition,2023-10-09,http://arxiv.org/abs/2310.05492,Guanting Dong; Hongyi Yuan; Keming Lu; Chengpeng Li; Mingfeng Xue; Dayiheng Liu; Wei Wang; Zheng Yuan; Chang Zhou; Jingren Zhou
A Theory for Emergence of Complex Skills in Language Models,2023-07-29,http://arxiv.org/abs/2307.15936,Sanjeev Arora; Anirudh Goyal
On the Power of Foundation Models,2023-07-03,https://proceedings.mlr.press/v202/yuan23b.html,Yang Yuan
Task-Specific Skill Localization in Fine-tuned Language Models,2023-06-15,https://openreview.net/forum?id=Rgnaj43Pk0,Abhishek Panigrahi; Nikunj Saunshi; Haoyu Zhao; Sanjeev Arora
Towards Understanding Why Mask-Reconstruction Pretraining Helps in Downstream Tasks,2023-02-11,http://arxiv.org/abs/2206.03826,Jiachun Pan; Pan Zhou; Shuicheng Yan
"Same Pre-training Loss, Better Downstream: Implicit Bias Matters for Language Models",2022-10-25,http://arxiv.org/abs/2210.14199,Hong Liu; Sang Michael Xie; Zhiyuan Li; Tengyu Ma
Why Do Pretrained Language Models Help in Downstream Tasks? An Analysis of Head and Prompt Tuning,2022-04-20,http://arxiv.org/abs/2106.09226,Colin Wei; Sang Michael Xie; Tengyu Ma
A Mathematical Exploration of Why Language Models Help Solve Downstream Tasks,2021-04-14,http://arxiv.org/abs/2010.03648,Nikunj Saunshi; Sadhika Malladi; Sanjeev Arora
Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning,2020-12-22,http://arxiv.org/abs/2012.13255,Armen Aghajanyan; Luke Zettlemoyer; Sonal Gupta
How fine can fine-tuning be? Learning efficient language models,2020-06-03,https://proceedings.mlr.press/v108/radiya-dixit20a.html,Evani Radiya-Dixit; Xin Wang
Initialization is Critical to Whether Transformers Fit Composite Functions by Inference or Memorizing,2024-05-08,http://arxiv.org/abs/2405.05409,Zhongwang Zhang; Pengxiao Lin; Zhiwei Wang; Yaoyu Zhang; Zhi-Qin John Xu
Provably learning a multi-head attention layer,2024-02-06,http://arxiv.org/abs/2402.04084,Sitan Chen; Yuanzhi Li
On the Optimization and Generalization of Multi-head Attention,2023-10-19,http://arxiv.org/abs/2310.12680,Puneesh Deora; Rouzbeh Ghaderi; Hossein Taheri; Christos Thrampoulidis
Quantifying the Gain in Weak-to-Strong Generalization,2024-05-24,http://arxiv.org/abs/2405.15116,Moses Charikar; Chirag Pabbaraju; Kirankumar Shiragur
Towards Understanding How Transformer Perform Multi-step Reasoning with Matching Operation,2024-05-24,http://arxiv.org/abs/2405.15302,Zhiwei Wang; Yunji Wang; Zhongwang Zhang; Zhangchen Zhou; Hui Jin; Tianyang Hu; Jiacheng Sun; Zhenguo Li; Yaoyu Zhang; Zhi-Qin John Xu
When can transformers reason with abstract symbols?,2024-04-16,http://arxiv.org/abs/2310.09753,Enric Boix-Adsera; Omid Saremi; Emmanuel Abbe; Samy Bengio; Etai Littwin; Joshua Susskind
Large Language Models Cannot Self-Correct Reasoning Yet,2023-10-13,https://openreview.net/forum?id=IkmD3fKBPQ,Jie Huang; Xinyun Chen; Swaroop Mishra; Huaixiu Steven Zheng; Adams Wei Yu; Xinying Song; Denny Zhou
A Mechanistic Analysis of a Transformer Trained on a Symbolic Multi-Step Reasoning Task,2024-02-19,http://arxiv.org/abs/2402.11917,Jannik Brinkmann; Abhay Sheshadri; Victor Levoso; Paul Swoboda; Christian Bartelt
A statistical framework for weak-to-strong generalization,2024-05-25,http://arxiv.org/abs/2405.16236,Seamus Somerstep; Felipe Maia Polo; Moulinath Banerjee; Ya'acov Ritov; Mikhail Yurochkin; Yuekai Sun
Reality Only Happens Once: Single-Path Generalization Bounds for Transformers,2024-05-26,http://arxiv.org/abs/2405.16563,Yannick Limmer; Anastasis Kratsios; Xuwei Yang; Raeid Saqur; Blanka Horvath
Theoretical Analysis of Weak-to-Strong Generalization,2024-05-25,http://arxiv.org/abs/2405.16043,Hunter Lang; David Sontag; Aravindan Vijayaraghavan
Understanding Transformer Reasoning Capabilities via Graph Algorithms,2024-05-28,http://arxiv.org/abs/2405.18512,Clayton Sanford; Bahare Fatemi; Ethan Hall; Anton Tsitsulin; Mehran Kazemi; Jonathan Halcrow; Bryan Perozzi; Vahab Mirrokni
Linguistic Collapse: Neural Collapse in (Large) Language Models,2024-05-28,https://arxiv.org/abs/2405.17767,Robert Wu; Vardan Papyan
How Truncating Weights Improves Reasoning in Language Models,2024-06-05,http://arxiv.org/abs/2406.03068,Lei Chen; Joan Bruna; Alberto Bietti
Learning on Transformers is Provable Low-Rank and Sparse: A One-layer Analysis,2024-06-24,http://arxiv.org/abs/2406.17167,Hongkang Li; Meng Wang; Shuai Zhang; Sijia Liu; Pin-Yu Chen
Implicit meta-learning may lead language models to trust more reliable sources,2023-10-23,http://arxiv.org/abs/2310.15047,Dmitrii Krasheninnikov; Egor Krasheninnikov; Bruno Mlodozeniec; Tegan Maharaj; David Krueger
On Initialization of Transformers with Pre-trained Embeddings,2024-07-17,http://arxiv.org/abs/2407.12514,Ha Young Kim; Niranjan Balasubramanian; Byungkon Kang
Unlocking Tokens as Data Points for Generalization Bounds on Larger Language Models,2024-07-25,http://arxiv.org/abs/2407.18158,Sanae Lotfi; Yilun Kuang; Brandon Amos; Micah Goldblum; Marc Finzi; Andrew Gordon Wilson
When can transformers compositionally generalize in-context?,2024-07-17,http://arxiv.org/abs/2407.12275,Seijin Kobayashi; Simon Schug; Yassir Akram; Florian Redhardt; Johannes von Oswald; Razvan Pascanu; Guillaume Lajoie; João Sacramento
Inductive or Deductive? Rethinking the Fundamental Reasoning Abilities of LLMs,2024-07-31,http://arxiv.org/abs/2408.00114,Kewei Cheng; Jingfeng Yang; Haoming Jiang; Zhengyang Wang; Binxuan Huang; Ruirui Li; Shiyang Li; Zheng Li; Yifan Gao; Xian Li; Bing Yin; Yizhou Sun
Out-of-distribution generalization via composition: a lens through induction heads in Transformers,2024-08-18,http://arxiv.org/abs/2408.09503,Jiajun Song; Zhuoyan Xu; Yiqiao Zhong
On the Generalization of Preference Learning with DPO,2024-08-06,http://arxiv.org/abs/2408.03459,Shawn Im; Yixuan Li
"Physics of Language Models: Part 2.1, Grade-School Math and the Hidden Reasoning Process",2024-07-29,http://arxiv.org/abs/2407.13123,Tian Ye; Zicheng Xu; Yuanzhi Li; Zeyuan Allen-Zhu
Reasoning in Large Language Models: A Geometric Perspective,2024-07-02,http://arxiv.org/abs/2407.02678,Romain Cosentino; Sarath Shekkizhar
Unforgettable Generalization in Language Models,2024-09-03,http://arxiv.org/abs/2409.02228,Eric Zhang; Leshem Chosen; Jacob Andreas
The Many Faces of Optimal Weak-to-Strong Learning,2024-08-30,http://arxiv.org/abs/2408.17148,Mikael Møller Høgsgaard; Kasper Green Larsen; Markus Engelund Mathiasen
On the Empirical Complexity of Reasoning and Planning in LLMs,2024-04-17,http://arxiv.org/abs/2404.11041,Liwei Kang; Zirui Zhao; David Hsu; Wee Sun Lee
Understanding Simplicity Bias towards Compositional Mappings via Learning Dynamics,2024-09-15,http://arxiv.org/abs/2409.09626,Yi Ren; Danica J. Sutherland
"Physics of Language Models: Part 2.2, How to Learn From Mistakes on Grade-School Math Problems",2024-08-29,http://arxiv.org/abs/2408.16293,Tian Ye; Zicheng Xu; Yuanzhi Li; Zeyuan Allen-Zhu
How Numerical Precision Affects Mathematical Reasoning Capabilities of LLMs,2024-10-17,http://arxiv.org/abs/2410.13857,Guhao Feng; Kai Yang; Yuntian Gu; Xinyue Ai; Shengjie Luo; Jiacheng Sun; Di He; Zhenguo Li; Liwei Wang
Benign or Not-Benign Overfitting in Token Selection of Attention Mechanism,2024-09-26,http://arxiv.org/abs/2409.17625,Keitaro Sakamoto; Issei Sato
Trained Transformer Classifiers Generalize and Exhibit Benign Overfitting In-Context,2024-10-02,http://arxiv.org/abs/2410.01774,Spencer Frei; Gal Vardi
Investigating the Impact of Model Complexity in Large Language Models,2024-10-01,http://arxiv.org/abs/2410.00699,Jing Luo; Huiyuan Wang; Weiran Huang
Lines of Thought in Large Language Models,2024-10-02,http://arxiv.org/abs/2410.01545,Raphaël Sarfati; Toni J. B. Liu; Nicolas Boullé; Christopher J. Earls
Benign Overfitting in Single-Head Attention,2024-10-10,http://arxiv.org/abs/2410.07746,Roey Magen; Shuning Shang; Zhiwei Xu; Spencer Frei; Wei Hu; Gal Vardi
Provable Weak-to-Strong Generalization via Benign Overfitting,2024-10-06,http://arxiv.org/abs/2410.04638,David X. Wu; Anant Sahai
A Formal Framework for Understanding Length Generalization in Transformers,2024-10-03,http://arxiv.org/abs/2410.02140,Xinting Huang; Andy Yang; Satwik Bhattamishra; Yash Sarrof; Andreas Krebs; Hattie Zhou; Preetum Nakkiran; Michael Hahn
Benign Overfitting for Regression with Trained Two-Layer ReLU Networks,2024-10-08,http://arxiv.org/abs/2410.06191,Junhyung Park; Patrick Bloebaum; Shiva Prasad Kasiviswanathan
Dynamics of Concept Learning and Compositional Generalization,2024-10-10,http://arxiv.org/abs/2410.08309,Yongyi Yang; Core Francisco Park; Ekdeep Singh Lubana; Maya Okawa; Wei Hu; Hidenori Tanaka
On Rank-Dependent Generalisation Error Bounds for Transformers,2024-10-15,http://arxiv.org/abs/2410.11500,Lan V. Truong
Mixture of Parrots: Experts improve memorization more than reasoning,2024-10-24,http://arxiv.org/abs/2410.19034,Samy Jelassi; Clara Mohri; David Brandfonbrener; Alex Gu; Nikhil Vyas; Nikhil Anand; David Alvarez-Melis; Yuanzhi Li; Sham M. Kakade; Eran Malach
RL-STaR: Theoretical Analysis of Reinforcement Learning Frameworks for Self-Taught Reasoner,2024-10-31,http://arxiv.org/abs/2410.23912,Fu-Chieh Chang; Yu-Ting Lee; Hui-Ying Shih; Pei-Yuan Wu
Generalization and Risk Bounds for Recurrent Neural Networks,2024-11-05,http://arxiv.org/abs/2411.02784,Xuewei Cheng; Ke Huang; Shujie Ma
Provable Length Generalization in Sequence Prediction via Spectral Filtering,2024-11-01,http://arxiv.org/abs/2411.01035,Annie Marsden; Evan Dogariu; Naman Agarwal; Xinyi Chen; Daniel Suo; Elad Hazan
