Title,Date,Url,Author
Algorithmic progress in language models,2024-03-09,http://arxiv.org/abs/2403.05812,Anson Ho; Tamay Besiroglu; Ege Erdil; David Owen; Robi Rahman; Zifan Carl Guo; David Atkinson; Neil Thompson; Jaime Sevilla
Massive Activations in Large Language Models,2024-02-27,http://arxiv.org/abs/2402.17762,Mingjie Sun; Xinlei Chen; J. Zico Kolter; Zhuang Liu
The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers,2023-02-01,https://openreview.net/forum?id=TJ2nxciYCk-,Zonglin Li; Chong You; Srinadh Bhojanapalli; Daliang Li; Ankit Singh Rawat; Sashank J. Reddi; Ke Ye; Felix Chern; Felix Yu; Ruiqi Guo; Sanjiv Kumar
The Platonic Representation Hypothesis,2024-05-13,https://arxiv.org/abs/2405.07987v1,Minyoung Huh; Brian Cheung; Tongzhou Wang; Phillip Isola
Your Transformer is Secretly Linear,2024-05-19,https://arxiv.org/abs/2405.12250,Anton Razzhigaev; Matvey Mikhalchuk; Elizaveta Goncharova; Nikolai Gerasimenko; Ivan Oseledets; Denis Dimitrov; Andrey Kuznetsov
Implicit Multimodal Alignment: On the Generalization of Frozen LLMs to Multimodal Inputs,2024-05-26,https://arxiv.org/abs/2405.16700,Mustafa Shukor; Matthieu Cord
Linguistic Collapse: Neural Collapse in (Large) Language Models,2024-05-28,https://arxiv.org/abs/2405.17767,Robert Wu; Vardan Papyan
Exploring Activation Patterns of Parameters in Language Models,2024-05-28,https://arxiv.org/abs/2405.17799,Yudong Wang; Damai Dai; Zhifang Sui
Anisotropy is Not Inherent to Transformers,2024-06,https://aclanthology.org/2024.naacl-long.274,Anemily Machina; Robert Mercer
Anisotropy Is Inherent to Self-Attention in Transformers,2024-01-24,http://arxiv.org/abs/2406.12143,Nathan Godey; Éric de la Clergerie; Benoît Sagot
Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data,2024-06-20,http://arxiv.org/abs/2406.14546,Johannes Treutlein; Dami Choi; Jan Betley; Cem Anil; Samuel Marks; Roger Baker Grosse; Owain Evans
Large Vocabulary Size Improves Large Language Models,2024-06-24,http://arxiv.org/abs/2406.16508,Sho Takase; Ryokan Ri; Shun Kiyono; Takuya Kato
Distributional reasoning in LLMs: Parallel reasoning processes in multi-hop reasoning,2024-06-19,http://arxiv.org/abs/2406.13858,Yuval Shalev; Amir Feder; Ariel Goldstein
Transcendence: Generative Models Can Outperform The Experts That Train Them,2024-06-17,http://arxiv.org/abs/2406.11741,Edwin Zhang; Vincent Zhu; Naomi Saphra; Anat Kleiman; Benjamin L. Edelman; Milind Tambe; Sham M. Kakade; Eran Malach
Taking a Deep Breath: Enhancing Language Modeling of Large Language Models with Sentinel Tokens,2024-06-16,http://arxiv.org/abs/2406.10985,Weiyao Luo; Suncong Zheng; Heming Xia; Weikang Wang; Yan Lei; Tianyu Liu; Shuang Chen; Zhifang Sui
Transformer Alignment in Large Language Models,2024-07-10,http://arxiv.org/abs/2407.07810,Murdock Aubry; Haoming Meng; Anton Sugolov; Vardan Papyan
Understanding Transformers via N-gram Statistics,2024-06-30,http://arxiv.org/abs/2407.12034,Timothy Nguyen
On the Benefits of Rank in Attention Layers,2024-07-23,http://arxiv.org/abs/2407.16153,Noah Amsel; Gilad Yehudai; Joan Bruna
Transformers on Markov Data: Constant Depth Suffices,2024-07-25,http://arxiv.org/abs/2407.17686,Nived Rajaraman; Marco Bondaschi; Kannan Ramchandran; Michael Gastpar; Ashok Vardhan Makkuva
By Tying Embeddings You Are Assuming the Distributional Hypothesis,2024-05-02,https://openreview.net/pdf?id=yyYMAprcAR,Francesco Bertolotti; Walter Cazzola
Emergent Representations of Program Semantics in Language Models Trained on Programs,2024-05-02,https://openreview.net/pdf?id=8PTx4CpNoT,Charles Jin; Martin Rinard
On the Emergence of Cross-Task Linearity in the Pretraining-Finetuning Paradigm,2024-02-06,http://arxiv.org/abs/2402.03660,Zhanpeng Zhou; Zijun Chen; Yilan Chen; Bo Zhang; Junchi Yan
Does Liking Yellow Imply Driving a School Bus? Semantic Leakage in Language Models,2024-08-12,http://arxiv.org/abs/2408.06518,Hila Gonen; Terra Blevins; Alisa Liu; Luke Zettlemoyer; Noah A. Smith
Large Language Monkeys: Scaling Inference Compute with Repeated Sampling,2024-07-31,http://arxiv.org/abs/2407.21787,Bradley Brown; Jordan Juravsky; Ryan Ehrlich; Ronald Clark; Quoc V. Le; Christopher Ré; Azalia Mirhoseini
Monotonic Representation of Numeric Properties in Language Models,2024-08-15,http://arxiv.org/abs/2408.10381,Benjamin Heinzerling; Kentaro Inui