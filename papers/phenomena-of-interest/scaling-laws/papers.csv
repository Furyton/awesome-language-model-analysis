Title,Date,Url,Author
More Compute Is What You Need,2024-04-30,http://arxiv.org/abs/2404.19484,Zhen Guo
An exactly solvable model for emergence and scaling laws,2024-04-26,http://arxiv.org/abs/2404.17563,Yoonsoo Nam; Nayara Fonseca; Seok Hyeong Lee; Ard Louis
Why do small language models underperform? Studying Language Model Saturation via the Softmax Bottleneck,2024-04-11,http://arxiv.org/abs/2404.07647,Nathan Godey; Éric de la Clergerie; Benoît Sagot
A Large-Scale Exploration of $\mu$-Transfer,2024-04-08,http://arxiv.org/abs/2404.05728,Lucas Lingle
Emergent Abilities in Reduced-Scale Generative Language Models,2024-04-02,http://arxiv.org/abs/2404.02204,Sherin Muckatira; Vijeta Deshpande; Vladislav Lialin; Anna Rumshisky
Understanding Emergent Abilities of Language Models from the Loss Perspective,2024-03-23,http://arxiv.org/abs/2403.15796,Zhengxiao Du; Aohan Zeng; Yuxiao Dong; Jie Tang
Unraveling the Mystery of Scaling Laws: Part I,2024-03-21,http://arxiv.org/abs/2403.06563,Hui Su; Zhi Tian; Xiaoyu Shen; Xunliang Cai
Language models scale reliably with over-training and on downstream tasks,2024-03-13,http://arxiv.org/abs/2403.08540,Samir Yitzhak Gadre; Georgios Smyrnis; Vaishaal Shankar; Suchin Gururangan; Mitchell Wortsman; Rulin Shao; Jean Mercat; Alex Fang; Jeffrey Li; Sedrick Keh; Rui Xin; Marianna Nezhurina; Igor Vasiljevic; Jenia Jitsev; Alexandros G. Dimakis; Gabriel Ilharco; Shuran Song; Thomas Kollar; Yair Carmon; Achal Dave; Reinhard Heckel; Niklas Muennighoff; Ludwig Schmidt
"When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method",2024-02-26,http://arxiv.org/abs/2402.17193,Biao Zhang; Zhongtao Liu; Colin Cherry; Orhan Firat
Interpreting Grokked Transformers in Complex Modular Arithmetic,2024-02-26,https://arxiv.org/abs/2402.16726v2,Hiroki Furuta; Gouki Minegishi; Yusuke Iwasawa; Yutaka Matsuo
Scaling Data-Constrained Language Models,2023-10-25,http://arxiv.org/abs/2305.16264,Niklas Muennighoff; Alexander M. Rush; Boaz Barak; Teven Le Scao; Aleksandra Piktus; Nouamane Tazi; Sampo Pyysalo; Thomas Wolf; Colin Raffel
The Cost of Down-Scaling Language Models: Fact Recall Deteriorates before In-Context Learning,2023-10-06,http://arxiv.org/abs/2310.04680,Tian Jin; Nolan Clement; Xin Dong; Vaishnavh Nagarajan; Michael Carbin; Jonathan Ragan-Kelley; Gintare Karolina Dziugaite
Are Emergent Abilities of Large Language Models a Mirage?,2023-04-28,https://arxiv.org/abs/2304.15004v2,Rylan Schaeffer; Brando Miranda; Sanmi Koyejo
Training Compute-Optimal Large Language Models,2022-03-29,http://arxiv.org/abs/2203.15556,Jordan Hoffmann; Sebastian Borgeaud; Arthur Mensch; Elena Buchatskaya; Trevor Cai; Eliza Rutherford; Diego de Las Casas; Lisa Anne Hendricks; Johannes Welbl; Aidan Clark; Tom Hennigan; Eric Noland; Katie Millican; George van den Driessche; Bogdan Damoc; Aurelia Guy; Simon Osindero; Karen Simonyan; Erich Elsen; Jack W. Rae; Oriol Vinyals; Laurent Sifre
Scaling Laws for Neural Language Models,2020-01-22,http://arxiv.org/abs/2001.08361,Jared Kaplan; Sam McCandlish; Tom Henighan; Tom B. Brown; Benjamin Chess; Rewon Child; Scott Gray; Alec Radford; Jeffrey Wu; Dario Amodei
A Tale of Tails: Model Collapse as a Change of Scaling Laws,2024-02-10,https://arxiv.org/abs/2402.07043,Elvis Dohmatob; Yunzhen Feng; Pu Yang; Francois Charton; Julia Kempe
Beyond Scaling Laws: Understanding Transformer Performance with Associative Memory,2024-05-14,http://arxiv.org/abs/2405.08707,Xueyan Niu; Bo Bai; Lei Deng; Wei Han
Quantifying Emergence in Large Language Models,2024-05-21,http://arxiv.org/abs/2405.12617,Hang Chen; Xinyu Yang; Jiaying Zhu; Wenya Wang
Slaves to the Law of Large Numbers: An Asymptotic Equipartition Property for Perplexity in Generative Language Models,2024-05-22,http://arxiv.org/abs/2405.13798,Raghu Mudumbai; Tyler Bell
Grokked Transformers are Implicit Reasoners: A Mechanistic Journey to the Edge of Generalization,2024-05-23,http://arxiv.org/abs/2405.15071,Boshi Wang; Xiang Yue; Yu Su; Huan Sun
Data Mixing Made Efficient: A Bivariate Scaling Law for Language Model Pretraining,2024-05-23,http://arxiv.org/abs/2405.14908,Ce Ge; Zhijian Ma; Daoyuan Chen; Yaliang Li; Bolin Ding
4+3 Phases of Compute-Optimal Neural Scaling Laws,2024-05-23,http://arxiv.org/abs/2405.15074,Elliot Paquette; Courtney Paquette; Lechao Xiao; Jeffrey Pennington
Emergence of a High-Dimensional Abstraction Phase in Language Transformers,2024-05-24,http://arxiv.org/abs/2405.15471,Emily Cheng; Diego Doimo; Corentin Kervadec; Iuri Macocco; Jade Yu; Alessandro Laio; Marco Baroni
gzip Predicts Data-dependent Scaling Laws,2024-05-26,http://arxiv.org/abs/2405.16684,Rohan Pandey
Deep Grokking: Would Deep Neural Networks Generalize Better?,2024-05-29,http://arxiv.org/abs/2405.19454,Simin Fan; Razvan Pascanu; Martin Jaggi
Linguistic Collapse: Neural Collapse in (Large) Language Models,2024-05-28,https://arxiv.org/abs/2405.17767,Robert Wu; Vardan Papyan
A rationale from frequency perspective for grokking in training neural network,2024-05-24,http://arxiv.org/abs/2405.17479,Zhangchen Zhou; Yaoyu Zhang; Zhi-Qin John Xu
Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations,2024-05-28,http://arxiv.org/abs/2405.18392,Alexander Hägele; Elie Bakouch; Atli Kosson; Loubna Ben Allal; Leandro Von Werra; Martin Jaggi
Scaling Laws for Linear Complexity Language Models,2024-06-24,http://arxiv.org/abs/2406.16690,Xuyang Shen; Dong Li; Ruitao Leng; Zhen Qin; Weigao Sun; Yiran Zhong
Scaling Laws for Fact Memorization of Large Language Models,2024-06-22,http://arxiv.org/abs/2406.15720,Xingyu Lu; Xiaonan Li; Qinyuan Cheng; Kai Ding; Xuanjing Huang; Xipeng Qiu
Reconciling Kaplan and Chinchilla Scaling Laws,2024-06-12,http://arxiv.org/abs/2406.12907,Tim Pearce; Jinyeop Song
Resolving Discrepancies in Compute-Optimal Scaling of Language Models,2024-06-25,http://arxiv.org/abs/2406.19146,Tomer Porian; Mitchell Wortsman; Jenia Jitsev; Ludwig Schmidt; Yair Carmon
Exploring Scaling Trends in LLM Robustness,2024-07-25,http://arxiv.org/abs/2407.18213,Nikolaus Howe; Michał Zajac; Ian McKenzie; Oskar Hollinsworth; Tom Tseng; Pierre-Luc Bacon; Adam Gleave
Emergence in non-neural models: grokking modular arithmetic via average gradient outer product,2024-07-29,http://arxiv.org/abs/2407.20199,Neil Mallinar; Daniel Beaglehole; Libin Zhu; Adityanarayanan Radhakrishnan; Parthe Pandit; Mikhail Belkin
Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies,2024-07-18,http://arxiv.org/abs/2407.13623,Chaofan Tao; Qian Liu; Longxu Dou; Niklas Muennighoff; Zhongwei Wan; Ping Luo; Min Lin; Ngai Wong
Why Do You Grok? A Theoretical Analysis of Grokking Modular Addition,2024-07-17,http://arxiv.org/abs/2407.12332,Mohamad Amin Mohamadi; Zhiyuan Li; Lei Wu; Danica J. Sutherland