Title,Date,Url,Author
More Compute Is What You Need,2024-04-30,http://arxiv.org/abs/2404.19484,Zhen Guo
An exactly solvable model for emergence and scaling laws,2024-04-26,http://arxiv.org/abs/2404.17563,Yoonsoo Nam; Nayara Fonseca; Seok Hyeong Lee; Ard Louis
Why do small language models underperform? Studying Language Model Saturation via the Softmax Bottleneck,2024-04-11,http://arxiv.org/abs/2404.07647,Nathan Godey; Éric de la Clergerie; Benoît Sagot
A Large-Scale Exploration of $\mu$-Transfer,2024-04-08,http://arxiv.org/abs/2404.05728,Lucas Lingle
Emergent Abilities in Reduced-Scale Generative Language Models,2024-04-02,http://arxiv.org/abs/2404.02204,Sherin Muckatira; Vijeta Deshpande; Vladislav Lialin; Anna Rumshisky
Understanding Emergent Abilities of Language Models from the Loss Perspective,2024-03-23,http://arxiv.org/abs/2403.15796,Zhengxiao Du; Aohan Zeng; Yuxiao Dong; Jie Tang
Unraveling the Mystery of Scaling Laws: Part I,2024-03-21,http://arxiv.org/abs/2403.06563,Hui Su; Zhi Tian; Xiaoyu Shen; Xunliang Cai
Language models scale reliably with over-training and on downstream tasks,2024-03-13,http://arxiv.org/abs/2403.08540,Samir Yitzhak Gadre; Georgios Smyrnis; Vaishaal Shankar; Suchin Gururangan; Mitchell Wortsman; Rulin Shao; Jean Mercat; Alex Fang; Jeffrey Li; Sedrick Keh; Rui Xin; Marianna Nezhurina; Igor Vasiljevic; Jenia Jitsev; Alexandros G. Dimakis; Gabriel Ilharco; Shuran Song; Thomas Kollar; Yair Carmon; Achal Dave; Reinhard Heckel; Niklas Muennighoff; Ludwig Schmidt
"When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method",2024-02-26,http://arxiv.org/abs/2402.17193,Biao Zhang; Zhongtao Liu; Colin Cherry; Orhan Firat
Interpreting Grokked Transformers in Complex Modular Arithmetic,2024-02-26,https://arxiv.org/abs/2402.16726v2,Hiroki Furuta; Gouki Minegishi; Yusuke Iwasawa; Yutaka Matsuo
Scaling Data-Constrained Language Models,2023-10-25,http://arxiv.org/abs/2305.16264,Niklas Muennighoff; Alexander M. Rush; Boaz Barak; Teven Le Scao; Aleksandra Piktus; Nouamane Tazi; Sampo Pyysalo; Thomas Wolf; Colin Raffel
The Cost of Down-Scaling Language Models: Fact Recall Deteriorates before In-Context Learning,2023-10-06,http://arxiv.org/abs/2310.04680,Tian Jin; Nolan Clement; Xin Dong; Vaishnavh Nagarajan; Michael Carbin; Jonathan Ragan-Kelley; Gintare Karolina Dziugaite
Are Emergent Abilities of Large Language Models a Mirage?,2023-04-28,https://arxiv.org/abs/2304.15004v2,Rylan Schaeffer; Brando Miranda; Sanmi Koyejo
Training Compute-Optimal Large Language Models,2022-03-29,http://arxiv.org/abs/2203.15556,Jordan Hoffmann; Sebastian Borgeaud; Arthur Mensch; Elena Buchatskaya; Trevor Cai; Eliza Rutherford; Diego de Las Casas; Lisa Anne Hendricks; Johannes Welbl; Aidan Clark; Tom Hennigan; Eric Noland; Katie Millican; George van den Driessche; Bogdan Damoc; Aurelia Guy; Simon Osindero; Karen Simonyan; Erich Elsen; Jack W. Rae; Oriol Vinyals; Laurent Sifre
Scaling Laws for Neural Language Models,2020-01-22,http://arxiv.org/abs/2001.08361,Jared Kaplan; Sam McCandlish; Tom Henighan; Tom B. Brown; Benjamin Chess; Rewon Child; Scott Gray; Alec Radford; Jeffrey Wu; Dario Amodei
A Tale of Tails: Model Collapse as a Change of Scaling Laws,2024-02-10,https://arxiv.org/abs/2402.07043,Elvis Dohmatob; Yunzhen Feng; Pu Yang; Francois Charton; Julia Kempe
