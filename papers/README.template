# Awesome Transformers LM Analytics [![Awesome](https://awesome.re/badge-flat2.svg)](https://awesome.re)

This paper list focuses on the **theoretical and empirical analysis** of language models, especially **large language models** (LLMs).
The papers in this list investigate the learning behavior, generalization ability, and other properties of language models through theoretical analysis, empirical analysis, or a combination of both.

Scope of this list:
- Currently, this list focuses on **transformer-based** models.
- We hope to collect papers that only focus on the theoretical and empirical analysis of language models, instead of papers that aim to improve the performance of language models.

Limitations of this list:
- This list is not exhaustive, and we may miss some very important papers.
- This list is not well-organized yet, and we may need to reorganize the list in the future.
- Some popular topics are not well-covered yet, such as mechanistic engineering, probing, and interpretability.

If you have any suggestions or want to contribute, please feel free to open an issue or a pull request.

For details on how to contribute, please refer to the [contribution guidelines](CONTRIBUTING.md).

Table of Content
====================
- [Awesome Transformers LM Analytics ](#awesome-transformers-lm-analytics-)
- [Table of Content](#table-of-content)
  - [Phenomena of Interest](#phenomena-of-interest)
    - [In-Context Learning](#in-context-learning)
    - [Chain-of-Thought](#chain-of-thought)
    - [Hallucination](#hallucination)
    - [Scaling Laws / Emergent Abilities / Grokking / etc.](#scaling-laws--emergent-abilities--grokking--etc)
    - [Knowledge / Memory mechanisms](#knowledge--memory-mechanisms)
    - [Training Dynamics / Landscape / Optimization / Fine-tuning / etc.](#training-dynamics--landscape--optimization--fine-tuning--etc)
    - [Learning / Generalization](#learning--generalization)
    - [Other Phenomena](#other-phenomena)
  - [Representational Capacity](#representational-capacity)
    - [What Can Transformer Do? / Properties of Transformer](#what-can-transformer-do--properties-of-transformer)
    - [What Can Transformer Not Do? / Limitation of Transformer](#what-can-transformer-not-do--limitation-of-transformer)
  - [Architectural Effectivity](#architectural-effectivity)
    - [Layer-normalization](#layer-normalization)
    - [Tokenization](#tokenization)
  - [Training Paradigms](#training-paradigms)
  - [Mechanistic Engineering / Probing / Interpretability](#mechanistic-engineering--probing--interpretability)
  - [Miscellanea](#miscellanea)

## Phenomena of Interest

Here are some phenomena that are interesting to investigate in language models.

### In-Context Learning

<details open>
<summary><em>paper list (click to fold / unfold)</em></summary>
<br>
{phenomena-of-interest/in-context-learning}

</details>

### Chain-of-Thought

<details open>
<summary><em>paper list (click to fold / unfold)</em></summary>
<br>
{phenomena-of-interest/chain-of-thought}

</details>

### Hallucination

<details open>
<summary><em>paper list (click to fold / unfold)</em></summary>
<br>
{phenomena-of-interest/hallucination}

</details>

### Scaling Laws / Emergent Abilities / Grokking / etc.

This section includes papers that investigate how the performance of language models scales with model size, data size, or compute, and how emergent abilities arise in language models.

<details open>
<summary><em>paper list (click to fold / unfold)</em></summary>
<br>
{phenomena-of-interest/scaling-laws}

</details>

### Knowledge / Memory mechanisms

<details open>
<summary><em>paper list (click to fold / unfold)</em></summary>
<br>
{phenomena-of-interest/knowledge}

</details>

### Training Dynamics / Landscape / Optimization / Fine-tuning / etc.

This section focuses on the training dynamics of language models, including the optimization landscape, fine-tuning, and transfer learning.

<details open>
<summary><em>paper list (click to fold / unfold)</em></summary>
<br>
{phenomena-of-interest/training-dynamics}

</details>

### Learning / Generalization

This section includes papers that investigate the generalization ability of language models, and the general learning behavior of language models.

<details open>
<summary><em>paper list (click to fold / unfold)</em></summary>
<br>
{phenomena-of-interest/learning}

</details>

### Other Phenomena

<details open>
<summary><em>paper list (click to fold / unfold)</em></summary>
<br>
{phenomena-of-interest/other-phenomena}

</details>

## Representational Capacity

Investigate the expressiveness of transformer-based models about what they can do and what they can't do.

### What Can Transformer Do? / Properties of Transformer

This section includes positive results on the representational capacity and properties of transformer-based models.

<details open>
<summary><em>paper list (click to fold / unfold)</em></summary>
<br>
{representational-capacity/what-can-transformer-do}

</details>

### What Can Transformer Not Do? / Limitation of Transformer

The papers in this section investigate the limitations of transformer-based models, including the limitations of their expressiveness and learning abilities.

<details open>
<summary><em>paper list (click to fold / unfold)</em></summary>
<br>
{representational-capacity/what-can-transformer-not-do}

</details>

## Architectural Effectivity

discussion of the effectiveness of different architectures in terms of learning and generalization

### Layer-normalization

<details open>
<summary><em>paper list (click to fold / unfold)</em></summary>
<br>
{architectural-effectivity/layer-normalization}

</details>

### Tokenization

<details open>
<summary><em>paper list (click to fold / unfold)</em></summary>
<br>
{architectural-effectivity/tokenization}

</details>

## Training Paradigms

<details open>
<summary><em>paper list (click to fold / unfold)</em></summary>
<br>
{training-paradigms}

</details>

## Mechanistic Engineering / Probing / Interpretability

This section includes papers that mainly investigate the mechanisms of language models through probing, mechanistic engineering, and other papers generally related to interpretability.

<details open>
<summary><em>paper list (click to fold / unfold)</em></summary>
<br>
{mechanistic-engineering}

</details>

## Miscellanea

<details open>
<summary><em>paper list (click to fold / unfold)</em></summary>
<br>
{miscellanea}

</details>

---

Contact:

- [Shiguang Wu](https://github.com/Furyton), furyton AT outlook.com / shiguang.wu AT mail.sdu.edu.cn