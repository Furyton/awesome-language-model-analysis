Title,Date,Url,Author
Compression Represents Intelligence Linearly,2024-04-15,http://arxiv.org/abs/2404.09937,Yuzhen Huang; Jinghan Zhang; Zifei Shan; Junxian He
Language Generation in the Limit,2024-04-10,http://arxiv.org/abs/2404.06757,Jon Kleinberg; Sendhil Mullainathan
Do language models plan ahead for future tokens?,2024-03-31,http://arxiv.org/abs/2404.00859,Wilson Wu; John X. Morris; Lionel Levine
Universality and Limitations of Prompt Tuning,2023-11-16,http://arxiv.org/abs/2305.18787,Yihan Wang; Jatin Chauhan; Wei Wang; Cho-Jui Hsieh
Data Similarity is Not Enough to Explain Language Model Performance,2023-11-15,http://arxiv.org/abs/2311.09006,Gregory Yauney; Emily Reif; David Mimno
Simplifying Transformer Blocks,2023-11-03,http://arxiv.org/abs/2311.01906,Bobby He; Thomas Hofmann
Causal Interpretation of Self-Attention in Pre-Trained Transformers,2023-10-31,http://arxiv.org/abs/2310.20307,Raanan Y. Rohekar; Yaniv Gurwicz; Shami Nisimov
How do Language Models Bind Entities in Context?,2023-10-26,http://arxiv.org/abs/2310.17191,Jiahai Feng; Jacob Steinhardt
Understanding prompt engineering may not require rethinking generalization,2023-10-13,https://openreview.net/forum?id=a745RnSFLT,Victor Akinwande; Yiding Jiang; Dylan Sam; J. Zico Kolter
Understanding Catastrophic Forgetting in Language Models via Implicit Inference,2023-09-18,http://arxiv.org/abs/2309.10105,Suhas Kotha; Jacob Mitchell Springer; Aditi Raghunathan
Attention-Only Transformers and Implementing MLPs with Attention Heads,2023-09-15,http://arxiv.org/abs/2309.08593,Robert Huben; Valerie Morris
On the Role of Attention in Prompt-tuning,2023-06-15,https://openreview.net/forum?id=qorOnDor89,Samet Oymak; Ankit Singh Rawat; Mahdi Soltanolkotabi; Christos Thrampoulidis
Understand LLMs Requires More Than Statistical Generalization,2024-05-03,http://arxiv.org/abs/2405.01964,Patrik Reizinger; Szilvia Ujváry; Anna Mészáros; Anna Kerekes; Wieland Brendel; Ferenc Huszár
What's In My Big Data?,2024-03-05,http://arxiv.org/abs/2310.20707,Yanai Elazar; Akshita Bhagia; Ian Magnusson; Abhilasha Ravichander; Dustin Schwenk; Alane Suhr; Pete Walsh; Dirk Groeneveld; Luca Soldaini; Sameer Singh; Hanna Hajishirzi; Noah A. Smith; Jesse Dodge
Dynamic Activation Pitfalls in LLaMA Models: An Empirical Study,2024-05-15,http://arxiv.org/abs/2405.09274,Chi Ma; Mincong Huang; Chao Wang; Yujie Wang; Lei Yu
Challenges in Deploying Long-Context Transformers: A Theoretical Peak Performance Analysis,2024-05-14,http://arxiv.org/abs/2405.08944,Yao Fu
"Surgical Feature-Space Decomposition of LLMs: Why, When and How?",2024-05-17,http://arxiv.org/abs/2405.13039,Arnav Chavan; Nahush Lele; Deepak Gupta
Attention as an RNN,2024-05-22,http://arxiv.org/abs/2405.13956,Leo Feng; Frederick Tung; Hossein Hajimirsadeghi; Mohamed Osama Ahmed; Yoshua Bengio; Greg Mori
Provably learning a multi-head attention layer,2024-02-06,http://arxiv.org/abs/2402.04084,Sitan Chen; Yuanzhi Li
Do Efficient Transformers Really Save Computation?,2024-02-21,http://arxiv.org/abs/2402.13934,Kai Yang; Jan Ackermann; Zhenyu He; Guhao Feng; Bohang Zhang; Yunzhen Feng; Qiwei Ye; Di He; Liwei Wang
Towards Understanding How Transformer Perform Multi-step Reasoning with Matching Operation,2024-05-24,http://arxiv.org/abs/2405.15302,Zhiwei Wang; Yunji Wang; Zhongwang Zhang; Zhangchen Zhou; Hui Jin; Tianyang Hu; Jiacheng Sun; Zhenguo Li; Yaoyu Zhang; Zhi-Qin John Xu
Long Is More for Alignment: A Simple but Tough-to-Beat Baseline for Instruction Fine-Tuning,2024-02-07,http://arxiv.org/abs/2402.04833,Hao Zhao; Maksym Andriushchenko; Francesco Croce; Nicolas Flammarion
Dissecting the Interplay of Attention Paths in a Statistical Mechanics Theory of Transformers,2024-05-24,http://arxiv.org/abs/2405.15926,Lorenzo Tiberi; Francesca Mignacco; Kazuki Irie; Haim Sompolinsky
Demystifying amortized causal discovery with transformers,2024-05-27,http://arxiv.org/abs/2405.16924,Francesco Montagna; Max Cairney-Leeming; Dhanya Sridhar; Francesco Locatello
Unlocking the Secrets of Linear Complexity Sequence Model from A Unified Perspective,2024-05-27,http://arxiv.org/abs/2405.17383,Zhen Qin; Xuyang Shen; Dong Li; Weigao Sun; Stan Birchfield; Richard Hartley; Yiran Zhong
Can Large Language Models Faithfully Express Their Intrinsic Uncertainty in Words?,2024-05-27,http://arxiv.org/abs/2405.16908,Gal Yona; Roee Aharoni; Mor Geva
A Theory of In-Context Learning in Transformers,2024-05-29,http://arxiv.org/abs/2405.18634,Yifei Wang; Yuyang Wu; Zeming Wei; Stefanie Jegelka; Yisen Wang
Lower Bounds on the Expressivity of Recurrent Neural Language Models,2024-05-29,http://arxiv.org/abs/2405.19222,Anej Svete; Franz Nowak; Anisha Mohamed Sahabdeen; Ryan Cotterell
Interpretability of Language Models via Task Spaces,2024-06-10,http://arxiv.org/abs/2406.06441,Lucas Weber; Jaap Jumelet; Elia Bruni; Dieuwke Hupkes
How Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States,2024-06-09,http://arxiv.org/abs/2406.05644,Zhenhong Zhou; Haiyang Yu; Xinghua Zhang; Rongwu Xu; Fei Huang; Yongbin Li
Attention as a Hypernetwork,2024-06-09,http://arxiv.org/abs/2406.05816,Simon Schug; Seijin Kobayashi; Yassir Akram; João Sacramento; Razvan Pascanu
Verbalized Machine Learning: Revisiting Machine Learning with Language Models,2024-06-06,http://arxiv.org/abs/2406.04344,Tim Z. Xiao; Robert Bamler; Bernhard Schölkopf; Weiyang Liu
Local to Global: Learning Dynamics and Effect of Initialization for Transformers,2024-06-05,http://arxiv.org/abs/2406.03072,Ashok Vardhan Makkuva; Marco Bondaschi; Chanakya Ekbote; Adway Girish; Alliot Nagle; Hyeji Kim; Michael Gastpar
Pre-trained Large Language Models Use Fourier Features to Compute Addition,2024-06-05,http://arxiv.org/abs/2406.03445,Tianyi Zhou; Deqing Fu; Vatsal Sharan; Robin Jia
LongSSM: On the Length Extension of State-space Models in Language Modelling,2024-06-04,http://arxiv.org/abs/2406.02080,Shida Wang
Computational Limits of Low-Rank Adaptation (LoRA) for Transformer-Based Models,2024-06-05,http://arxiv.org/abs/2406.03136,Jerry Yao-Chieh Hu; Maojiang Su; En-Jui Kuo; Zhao Song; Han Liu
On Affine Homotopy between Language Encoders,2024-06-04,http://arxiv.org/abs/2406.02329,Robin SM Chan; Reda Boumasmoud; Anej Svete; Yuxin Ren; Qipeng Guo; Zhijing Jin; Shauli Ravfogel; Mrinmaya Sachan; Bernhard Schölkopf; Mennatallah El-Assady; Ryan Cotterell
Rethinking Spiking Neural Networks as State Space Models,2024-06-05,http://arxiv.org/abs/2406.02923,Malyaban Bal; Abhronil Sengupta
Understanding Jailbreak Success: A Study of Latent Space Dynamics in Large Language Models,2024-06-13,http://arxiv.org/abs/2406.09289,Sarah Ball; Frauke Kreuter; Nina Rimsky
Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data,2024-06-20,http://arxiv.org/abs/2406.14546,Johannes Treutlein; Dami Choi; Jan Betley; Cem Anil; Samuel Marks; Roger Baker Grosse; Owain Evans
Demystifying Forgetting in Language Model Fine-Tuning with Statistical Analysis of Example Associations,2024-06-20,http://arxiv.org/abs/2406.14026,Xisen Jin; Xiang Ren
On Layer-wise Representation Similarity: Application for Multi-Exit Models with a Single Classifier,2024-06-20,http://arxiv.org/abs/2406.14479,Jiachen Jiang; Jinxin Zhou; Zhihui Zhu
Unveiling the Hidden Structure of Self-Attention via Kernel Principal Component Analysis,2024-06-19,http://arxiv.org/abs/2406.13762,Rachel S.Y. Teo; Tan M. Nguyen
Evaluating n-Gram Novelty of Language Models Using Rusty-DAWG,2024-06-25,http://arxiv.org/abs/2406.13069,William Merrill; Noah A. Smith; Yanai Elazar
A Text is Worth Several Tokens: Text Embedding from LLMs Secretly Aligns Well with The Key Tokens,2024-06-25,http://arxiv.org/abs/2406.17378,Zhijie Nie; Richong Zhang; Zhanyu Wu
How to Compute the Probability of a Word,2024-06-20,http://arxiv.org/abs/2406.14561,Tiago Pimentel; Clara Meister
Anisotropy is Not Inherent to Transformers,2024-06,https://aclanthology.org/2024.naacl-long.274,Anemily Machina; Robert Mercer
Anisotropy Is Inherent to Self-Attention in Transformers,2024-01-24,http://arxiv.org/abs/2406.12143,Nathan Godey; Éric de la Clergerie; Benoît Sagot
Attention Score is not All You Need for Token Importance Indicator in KV Cache Reduction: Value Also Matters,2024-06-18,http://arxiv.org/abs/2406.12335,Zhiyu Guo; Hidetaka Kamigaito; Taro Watanabe
Exploring the Impact of a Transformer's Latent Space Geometry on Downstream Task Performance,2024-06-18,http://arxiv.org/abs/2406.12159,Anna C. Marbut; John W. Chandler; Travis J. Wheeler
Toward Infinite-Long Prefix in Transformer,2024-06-20,http://arxiv.org/abs/2406.14036,Jiuxiang Gu; Yingyu Liang; Zhenmei Shi; Zhao Song; Chiwun Yang
Textual Unlearning Gives a False Sense of Unlearning,2024-06-19,http://arxiv.org/abs/2406.13348,Jiacheng Du; Zhibo Wang; Kui Ren
Universal Length Generalization with Turing Programs,2024-07-03,http://arxiv.org/abs/2407.03310,Kaiying Hou; David Brandfonbrener; Sham Kakade; Samy Jelassi; Eran Malach
Toward Infinite-Long Prefix in Transformer,2024-06-20,http://arxiv.org/abs/2406.14036,Jiuxiang Gu; Yingyu Liang; Zhenmei Shi; Zhao Song; Chiwun Yang
Computational Limits of Low-Rank Adaptation (LoRA) for Transformer-Based Models,2024-06-05,http://arxiv.org/abs/2406.03136,Jerry Yao-Chieh Hu; Maojiang Su; En-Jui Kuo; Zhao Song; Han Liu
Fundamental Limits of Prompt Compression: A Rate-Distortion Framework for Black-Box Language Models,2024-07-22,http://arxiv.org/abs/2407.15504,Adway Girish; Alliot Nagle; Marco Bondaschi; Michael Gastpar; Ashok Vardhan Makkuva; Hyeji Kim
In-Context Probing Approximates Influence Function for Data Valuation,2024-07-17,http://arxiv.org/abs/2407.12259,Cathy Jiao; Gary Gao; Chenyan Xiong
On Initialization of Transformers with Pre-trained Embeddings,2024-07-17,http://arxiv.org/abs/2407.12514,Ha Young Kim; Niranjan Balasubramanian; Byungkon Kang
On Exact Bit-level Reversible Transformers Without Changing Architectures,2024-07-12,http://arxiv.org/abs/2407.09093,Guoqiang Zhang; J.P. Lewis; W. B. Kleijn
Understanding Transformers via N-gram Statistics,2024-06-30,http://arxiv.org/abs/2407.12034,Timothy Nguyen
Efficient Training of Language Models with Compact and Consistent Next Token Distributions,2024-07-03,http://arxiv.org/abs/2407.02819,Ashutosh Sathe; Sunita Sarawagi
On the Benefits of Rank in Attention Layers,2024-07-23,http://arxiv.org/abs/2407.16153,Noah Amsel; Gilad Yehudai; Joan Bruna
Data Debugging is NP-hard for Classifiers Trained with SGD,2024-08-02,http://arxiv.org/abs/2408.01365,Zizheng Guo; Pengyu Chen; Yanzhang Fu; Dongjing Miao
Dancing in Chains: Reconciling Instruction Following and Faithfulness in Language Models,2024-07-31,http://arxiv.org/abs/2407.21417,Zhengxuan Wu; Yuhao Zhang; Peng Qi; Yumo Xu; Rujun Han; Yian Zhang; Jifan Chen; Bonan Min; Zhiheng Huang
Language Models as Models of Language,2024-08-13,http://arxiv.org/abs/2408.07144,Raphaël Millière
Learning Randomized Algorithms with Transformers,2024-08-20,http://arxiv.org/abs/2408.10818,Johannes von Oswald; Seijin Kobayashi; Yassir Akram; Angelika Steger
Enhancing Exploratory Learning through Exploratory Search with the Emergence of Large Language Models,2024-08-09,http://arxiv.org/abs/2408.08894,Yiming Luo; Patrick Cheong-Iao; Shanton Chang
"Great Memory, Shallow Reasoning: Limits of kNN-LMs",2024-08-21,http://arxiv.org/abs/2408.11815,Shangyi Geng; Wenting Zhao; Alexander M Rush
A Tighter Complexity Analysis of SparseGPT,2024-08-22,http://arxiv.org/abs/2408.12151,Xiaoyu Li; Yingyu Liang; Zhenmei Shi; Zhao Song
Multi-Layer Transformers Gradient Can be Approximated in Almost Linear Time,2024-08-23,http://arxiv.org/abs/2408.13233,Yingyu Liang; Zhizhou Sha; Zhenmei Shi; Zhao Song; Yufa Zhou
Viewing Transformers Through the Lens of Long Convolutions Layers,2024-05-02,http://openreview.net/pdf?id=nOyj26YdIQ,Itamar Zimerman; Lior Wolf
Modeling Language Tokens as Functionals of Semantic Fields,2024-05-02,http://openreview.net/pdf?id=EEO4Iktfjp,Zhengqi Pei; Anran Zhang; Shuhui Wang; Qingming Huang