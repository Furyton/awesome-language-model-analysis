Title,Date,Url,Author
Compression Represents Intelligence Linearly,2024-04-15,http://arxiv.org/abs/2404.09937,Yuzhen Huang; Jinghan Zhang; Zifei Shan; Junxian He
Language Generation in the Limit,2024-04-10,http://arxiv.org/abs/2404.06757,Jon Kleinberg; Sendhil Mullainathan
Do language models plan ahead for future tokens?,2024-03-31,http://arxiv.org/abs/2404.00859,Wilson Wu; John X. Morris; Lionel Levine
Universality and Limitations of Prompt Tuning,2023-11-16,http://arxiv.org/abs/2305.18787,Yihan Wang; Jatin Chauhan; Wei Wang; Cho-Jui Hsieh
Data Similarity is Not Enough to Explain Language Model Performance,2023-11-15,http://arxiv.org/abs/2311.09006,Gregory Yauney; Emily Reif; David Mimno
Simplifying Transformer Blocks,2023-11-03,http://arxiv.org/abs/2311.01906,Bobby He; Thomas Hofmann
Causal Interpretation of Self-Attention in Pre-Trained Transformers,2023-10-31,http://arxiv.org/abs/2310.20307,Raanan Y. Rohekar; Yaniv Gurwicz; Shami Nisimov
How do Language Models Bind Entities in Context?,2023-10-26,http://arxiv.org/abs/2310.17191,Jiahai Feng; Jacob Steinhardt
Understanding prompt engineering may not require rethinking generalization,2023-10-13,https://openreview.net/forum?id=a745RnSFLT,Victor Akinwande; Yiding Jiang; Dylan Sam; J. Zico Kolter
Understanding Catastrophic Forgetting in Language Models via Implicit Inference,2023-09-18,http://arxiv.org/abs/2309.10105,Suhas Kotha; Jacob Mitchell Springer; Aditi Raghunathan
Attention-Only Transformers and Implementing MLPs with Attention Heads,2023-09-15,http://arxiv.org/abs/2309.08593,Robert Huben; Valerie Morris
On the Role of Attention in Prompt-tuning,2023-06-15,https://openreview.net/forum?id=qorOnDor89,Samet Oymak; Ankit Singh Rawat; Mahdi Soltanolkotabi; Christos Thrampoulidis
Understand LLMs Requires More Than Statistical Generalization,2024-05-03,http://arxiv.org/abs/2405.01964,Patrik Reizinger; Szilvia Ujváry; Anna Mészáros; Anna Kerekes; Wieland Brendel; Ferenc Huszár
What's In My Big Data?,2024-03-05,http://arxiv.org/abs/2310.20707,Yanai Elazar; Akshita Bhagia; Ian Magnusson; Abhilasha Ravichander; Dustin Schwenk; Alane Suhr; Pete Walsh; Dirk Groeneveld; Luca Soldaini; Sameer Singh; Hanna Hajishirzi; Noah A. Smith; Jesse Dodge
Dynamic Activation Pitfalls in LLaMA Models: An Empirical Study,2024-05-15,http://arxiv.org/abs/2405.09274,Chi Ma; Mincong Huang; Chao Wang; Yujie Wang; Lei Yu
Challenges in Deploying Long-Context Transformers: A Theoretical Peak Performance Analysis,2024-05-14,http://arxiv.org/abs/2405.08944,Yao Fu