Title,Date,Url,Author
Toward a Theory of Tokenization in LLMs,2024-04-12,http://arxiv.org/abs/2404.08335,Nived Rajaraman; Jiantao Jiao; Kannan Ramchandran
On the Effect of (Near) Duplicate Subwords in Language Modelling,2024-04-09,http://arxiv.org/abs/2404.06508,Anton Schäfer; Thomas Hofmann; Imanol Schlag; Tiago Pimentel
Tokenization Is More Than Compression,2024-02-28,http://arxiv.org/abs/2402.18376,Craig W. Schmidt; Varshini Reddy; Haoran Zhang; Alec Alameddine; Omri Uzan; Yuval Pinter; Chris Tanner
Understanding and Mitigating Tokenization Bias in Language Models,2024-06-24,http://arxiv.org/abs/2406.16829,Buu Phan; Marton Havasi; Matthew Muckley; Karen Ullrich
Large Vocabulary Size Improves Large Language Models,2024-06-24,http://arxiv.org/abs/2406.16508,Sho Takase; Ryokan Ri; Shun Kiyono; Takuya Kato
Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies,2024-07-18,http://arxiv.org/abs/2407.13623,Chaofan Tao; Qian Liu; Longxu Dou; Niklas Muennighoff; Zhongwei Wan; Ping Luo; Min Lin; Ngai Wong
By Tying Embeddings You Are Assuming the Distributional Hypothesis,2024-05-02,https://openreview.net/pdf?id=yyYMAprcAR,Francesco Bertolotti; Walter Cazzola
Transformers Can Do Arithmetic with the Right Embeddings,2024-05-27,http://arxiv.org/abs/2405.17399,Sean McLeish; Arpit Bansal; Alex Stein; Neel Jain; John Kirchenbauer; Brian R. Bartoldson; Bhavya Kailkhura; Abhinav Bhatele; Jonas Geiping; Avi Schwarzschild; Tom Goldstein
Small Transformers Compute Universal Metric Embeddings,2022-10-18,http://arxiv.org/abs/2209.06788,Anastasis Kratsios; Valentin Debarnot; Ivan Dokmanić
On Initialization of Transformers with Pre-trained Embeddings,2024-07-17,http://arxiv.org/abs/2407.12514,Ha Young Kim; Niranjan Balasubramanian; Byungkon Kang
Where is the signal in tokenization space?,2024-08-16,http://arxiv.org/abs/2408.08541,Renato Lui Geh; Honghua Zhang; Kareem Ahmed; Benjie Wang; Guy Van den Broeck
Reconsidering Token Embeddings with the Definitions for Pre-trained Language Models,2024-08-02,http://arxiv.org/abs/2408.01308,Ying Zhang; Dongyuan Li; Manabu Okumura
Monotonic Representation of Numeric Properties in Language Models,2024-08-15,http://arxiv.org/abs/2408.10381,Benjamin Heinzerling; Kentaro Inui
Where is the signal in tokenization space?,2024-08-16,http://arxiv.org/abs/2408.08541,Renato Lui Geh; Honghua Zhang; Kareem Ahmed; Benjie Wang; Guy Van den Broeck
Data Mixture Inference: What do BPE Tokenizers Reveal about their Training Data?,2024-07-23,http://arxiv.org/abs/2407.16607,Jonathan Hayase; Alisa Liu; Yejin Choi; Sewoong Oh; Noah A. Smith
An Empirical Comparison of Vocabulary Expansion and Initialization Approaches for Language Models,2024-07-08,http://arxiv.org/abs/2407.05841,Nandini Mundra; Aditya Nanda Kishore; Raj Dabre; Ratish Puduppully; Anoop Kunchukuttan; Mitesh M. Khapra
"Pooling And Attention: What Are Effective Designs For LLm-Based Embedding Models?",2024-09-04,https://arxiv.org/pdf/2409.02727,Yixuan Tang, Yi Yang
